/*
 * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
 * SPDX-License-Identifier: Apache-2.0
*/

// == crypto/fipsmodule/modes/gcm.c:71 ==
// #define GHASH_CHUNK (3 * 1024)
let GHASH_CHUNK = eval_size {| 3 * 1024 |};
let GHASH_CHUNK_BLOCKS = eval_size {| GHASH_CHUNK / AES_BLOCK_SIZE |};

let MAX_BLOCKS_AFTER_BULK = 18;

////////////////////////////////////////////////////////////////////////////////
// Rewrite rules specific to GHASH

let prove_gcm_ghash_avx_thm len n m = prove_theorem
  (do {
    goal_eval_unint ["pmult", "pmod", "gcm_polyval", "gcm_polyval_mul", "gcm_polyval_mul_pmult3", "gcm_polyval_mul_pmult4", "gcm_polyval_red", "gcm_polyval_red_pmult"];
    simplify (addsimps gcm_polyval_thms empty_ss);
    simplify (addsimps [concat_assoc_0_thm] empty_ss);
    w4_unint_yices ["pmult", "pmod", "gcm_polyval"];
  })
  (rewrite (cryptol_ss ()) {{ \H Xi (in : [len][8]) -> gcm_ghash H Xi in == gcm_ghash_avx`{n=n,m=m} H Xi in }});

// For our current proof, we will never need to process more than 18
// blocks after the bulk encryption phase, so that is all we need to
// do here.

gcm_ghash_avx_thms <-
  for (eval_list {{ [1..MAX_BLOCKS_AFTER_BULK:[16] ] }}) (\ i ->
   do { 
     let blocks = eval_int i;
     print (str_concat "gcm_ghash_avx_lemmma: " (show blocks));
     let len = eval_size {| blocks * AES_BLOCK_SIZE |};
     let groups = eval_size {| blocks/8 |};
     let m = eval_size {| blocks%8 |};
     if eval_bool {{ `m == 0 }} then
       prove_gcm_ghash_avx_thm len (eval_size {| groups-1 |}) 8
     else
       prove_gcm_ghash_avx_thm len groups m;
   });

gcm_ghash_array_thms <-
  for (eval_list {{ [1..MAX_BLOCKS_AFTER_BULK:[16] ] }}) (\ i ->
   do { 
     let blocks = eval_int i;
     print (str_concat "gcm_ghash_array_lemmma: " (show blocks));
     prove_theorem
       (w4_unint_z3 ["pmult", "gcm_ghash_block"])
       (rewrite (cryptol_ss ()) (unfold_term ["gcm_ghash_array_eq_property"] {{ gcm_ghash_array_eq_property`{blocks} }}));
   });

////////////////////////////////////////////////////////////////////////////////
// Specifications

/* == crypto/fipsmodule/modes/internal.h:272 ==
 * == generated-src/linux-x86_64/crypto/fipsmodule/ghash-x86_64.S ==
 *
 * void gcm_init_avx(u128 Htable[16], const uint64_t Xi[2]);
 */
let gcm_init_avx_spec = do {
  (_Htable, Htable_ptr) <- ptr_to_fresh "Htable" (llvm_array 12 (llvm_int 128));
  (Xi, Xi_ptr) <- ptr_to_fresh_readonly "Xi" (llvm_array 2 (llvm_int 64));

  crucible_execute_func [Htable_ptr, Xi_ptr];

  crucible_points_to Htable_ptr (crucible_term {{ gcm_init Xi }});
};

/* == crypto/fipsmodule/modes/internal.h:273 ==
 * == generated-src/linux-x86_64/crypto/fipsmodule/ghash-x86_64.S ==
 *
 * void gcm_gmult_avx(uint64_t Xi[2], const u128 Htable[16]);
 */
let gcm_gmult_avx_spec = do {
  (Xi, Xi_ptr) <- ptr_to_fresh "Xi" (llvm_array 16 (llvm_int 8));
  Htable_ptr <- crucible_alloc_readonly (llvm_array 12 (llvm_int 128));
  Htable0 <- crucible_fresh_var "Htable0" (llvm_int 128);
  crucible_points_to_untyped (crucible_elem Htable_ptr 1) (crucible_term {{ drop`{1} (gcm_init_Htable Htable0) }});
  crucible_points_to_untyped (crucible_elem Htable_ptr 0) (crucible_term {{ Htable0 }});

  crucible_execute_func [Xi_ptr, Htable_ptr];

  crucible_points_to Xi_ptr (crucible_term {{ gcm_gmult Htable0 Xi }});
};

/* == crypto/fipsmodule/modes/internal.h:274 ==
 * == generated-src/linux-x86_64/crypto/fipsmodule/ghash-x86_64.S ==
 *
 * void gcm_ghash_avx(uint64_t Xi[2], const u128 Htable[16], const uint8_t *in,
 *                    size_t len);
 */
let gcm_ghash_avx_concrete_spec blocks = do {
  let len = {{ `(blocks*AES_BLOCK_SIZE):[64] }};
  llvm_precond {{ len <= `GHASH_CHUNK }};

  (Xi, Xi_ptr) <- ptr_to_fresh "Xi" (llvm_array 16 (llvm_int 8));
  Htable0 <- crucible_fresh_var "Htable0" (llvm_int 128);
  Htable_ptr <- crucible_alloc_readonly (llvm_array 12 (llvm_int 128));

  crucible_points_to_untyped (crucible_elem Htable_ptr 0) (crucible_term {{ Htable0 }});
  crucible_points_to_untyped (crucible_elem Htable_ptr 1) (crucible_term {{ drop`{1} (gcm_init_Htable Htable0) }});

  (inp, inp_ptr) <- ptr_to_fresh_array_readonly "in" len;

  crucible_execute_func [Xi_ptr, Htable_ptr, inp_ptr, crucible_term len];

  crucible_points_to Xi_ptr (crucible_term {{ gcm_ghash_array Htable0 Xi inp 0 (`blocks:[64]) }});
};

let gcm_ghash_avx_spec = do {
  len <- llvm_fresh_var "len" (llvm_int 64);
  llvm_precond {{ len%16 == 0 }};
  llvm_precond {{ 0 < len }};
  llvm_precond {{ len < `(MAX_BLOCKS_AFTER_BULK*AES_BLOCK_SIZE) }};

  (Xi, Xi_ptr) <- ptr_to_fresh "Xi" (llvm_array 16 (llvm_int 8));
  Htable0 <- crucible_fresh_var "Htable0" (llvm_int 128);
  Htable_ptr <- crucible_alloc_readonly (llvm_array 12 (llvm_int 128));

  crucible_points_to_untyped (crucible_elem Htable_ptr 0) (crucible_term {{ Htable0 }});
  crucible_points_to_untyped (crucible_elem Htable_ptr 1) (crucible_term {{ drop`{1} (gcm_init_Htable Htable0) }});

  (inp, inp_ptr) <- ptr_to_fresh_array_readonly "in" len;

  crucible_execute_func [Xi_ptr, Htable_ptr, inp_ptr, crucible_term len];

  crucible_points_to Xi_ptr (crucible_term {{ gcm_ghash_array Htable0 Xi inp 0 (len/16) }});
};

// This does't seem to be working... there are actually two loops
// in this code, and getting the second one to symbolically terminate
// is problematic...

// let {{
//   type gcm_ghash_avx_invariant_tuple =
//     ( [512], [512], [512], [512], [512], [512], [512], [512], [512]
//     , [64], [64]
//     )

//   gcm_ghash_avx_invariant :
//     [128] ->
//     Array [64] [8] ->
//     gcm_ghash_avx_invariant_tuple ->
//     gcm_ghash_avx_invariant_tuple ->
//     Bool
//   gcm_ghash_avx_invariant H0 in
//     ( _, _, _, _, _, _, _, _, _, i_a, i_b )
//     ( _, _, _, _, _, _, _, _, _, c_a, c_b ) =

//     i_b >= c_b /\ i_a >= c_a
    
// }};

// asdfasd <- llvm_verify_fixpoint_x86_ex
//   m "../../build/x86/crypto/crypto_test" "gcm_ghash_avx"
//   [ ("gcm_ghash_avx", 1800) ]
//   true
//   ("gcm_ghash_avx", 0, {{ gcm_ghash_avx_invariant }})
//   gcm_ghash_avx_spec
//   do {
//     loop_inv0 <- goal_has_some_tag ["initial loop invariant"];
//     loop_inv  <- goal_has_some_tag ["inductive loop invariant"];
//     if loop_inv0 then do {
//       return (run (print "<<initial loop invariant>>"));
//     	 proof_subshell ();
//     } else if loop_inv then do {
//       return (run (print "<<inductive loop invariant>>"));
//     	 proof_subshell ();
//     } else do {
//       admit "Skipping";
//     };
//   };


////////////////////////////////////////////////////////////////////////////////
// Proof commands

enable_what4_hash_consing;

gcm_init_avx_ov <- llvm_verify_x86 m "../../build/x86/crypto/crypto_test" "gcm_init_avx"
  [ ("gcm_ghash_avx", 1800) // similar hack to the one in AESNI-GCM.saw to grab .L0x1c2_polynomial, we need a better way to handle this
  ]
  true
  gcm_init_avx_spec
  (do {
    unfolding ["gcm_init", "gcm_init_Htable"];
    simplify (addsimps [polyval_avx_thm] empty_ss);
    w4_unint_yices ["pmult"];
  });
disable_what4_hash_consing;


gcm_gmult_avx_ov <- llvm_verify_x86 m "../../build/x86/crypto/crypto_test" "gcm_gmult_avx"
  [ ("gcm_ghash_avx", 1800)
  ]
  true
  gcm_gmult_avx_spec
  rme;

enable_what4_hash_consing;

let gcm_ghash_avx_tactic = do {
  simplify (cryptol_ss ());
  simplify (addsimps gcm_ghash_array_thms empty_ss);
  simplify (addsimps gcm_ghash_avx_thms empty_ss);
  goal_eval_unint ["pmult", "gcm_polyval"];
  simplify (addsimps xor_slice_append_thms basic_ss);
  simplify (addsimps slice_slice_thms empty_ss);
  simplify (addsimps xor_slice_append_thms basic_ss);
  simplify (addsimps concat_assoc_0_thms empty_ss);
  simplify (addsimps concat_assoc_1_thms empty_ss);
  simplify (addsimps concat_assoc_2_thms empty_ss);
  w4_unint_z3 ["pmult", "gcm_polyval"];
};

gcm_ghash_avx_concrete_ovs <-
  for (eval_list {{ [1..MAX_BLOCKS_AFTER_BULK:[16]] }}) (\i -> do
    { let blocks = eval_int i;
      print (str_concat "gcm_ghash_avx concrete override: " (show blocks));
      if eval_bool {{ i > 18 }} then
        crucible_llvm_unsafe_assume_spec m "gcm_ghash_avx"
          (gcm_ghash_avx_concrete_spec blocks)
      else 
        llvm_verify_x86 m "../../build/x86/crypto/crypto_test" "gcm_ghash_avx"
          [ ("gcm_ghash_avx", 1800) ]
          true
          (gcm_ghash_avx_concrete_spec blocks)
          gcm_ghash_avx_tactic; });

disable_what4_hash_consing;

// Assemble the individual proofs at each concrete length
// into the overall specification
gcm_ghash_avx_ov <-
  // crucible_llvm_unsafe_assume_spec m "gcm_ghash_avx"
  //   gcm_ghash_avx_spec;

  llvm_refine_spec m "gcm_ghash_avx"
    gcm_ghash_avx_concrete_ovs
    gcm_ghash_avx_spec
    (w4_unint_z3 ["gcm_ghash_array","gcm_init_Htable"]);
