/*
 * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
 * SPDX-License-Identifier: Apache-2.0
*/

// == crypto/fipsmodule/modes/gcm.c:71 ==
// #define GHASH_CHUNK (3 * 1024)
let GHASH_CHUNK = eval_size {| 3 * 1024 |};
let GHASH_CHUNK_BLOCKS = eval_size {| GHASH_CHUNK / AES_BLOCK_SIZE |};

let MAX_BLOCKS_AFTER_BULK = 18;
// let MAX_BLOCKS_AFTER_BULK = 2;

////////////////////////////////////////////////////////////////////////////////
// Rewrite rules specific to GHASH

let prove_gcm_ghash_avx_thm len n m = prove_theorem
  (do {
    goal_eval_unint ["pmult", "pmod", "gcm_polyval", "gcm_polyval_mul", "gcm_polyval_mul_pmult3", "gcm_polyval_mul_pmult4", "gcm_polyval_red", "gcm_polyval_red_pmult"];
    simplify (addsimps gcm_polyval_thms empty_ss);
    simplify (addsimps [concat_assoc_0_thm] empty_ss);
    w4_unint_yices ["pmult", "pmod", "gcm_polyval"];
  })
  (rewrite (cryptol_ss ()) {{ \H Xi (in : [len][8]) -> gcm_ghash H Xi in == gcm_ghash_avx`{n=n,m=m} H Xi in }});

// For our current proof, we will never need to process more than 17
// blocks after the bulk encryption phase, so that is all we need to
// do here.
gcm_ghash_avx_thms <-
  for (eval_list {{ [1..MAX_BLOCKS_AFTER_BULK:[16] ] }}) (\ i ->
   do { 
     let blocks = eval_int i;
     print (str_concat "gcm_ghash_avx_lemmma: " (show blocks));
     let len = eval_size {| blocks * AES_BLOCK_SIZE |};
     let groups = eval_size {| blocks/8 |};
     let m = eval_size {| blocks%8 |};
     if eval_bool {{ `m == 0 }} then
       prove_gcm_ghash_avx_thm len (eval_size {| groups-1 |}) 8
     else
       prove_gcm_ghash_avx_thm len groups m;
   });

gcm_ghash_array_thms <-
  for (eval_list {{ [1..MAX_BLOCKS_AFTER_BULK:[16] ] }}) (\ i ->
   do { 
     let blocks = eval_int i;
     print (str_concat "gcm_ghash_array_lemmma: " (show blocks));
     prove_theorem
       (w4_unint_z3 ["pmult", "gcm_ghash_block"])
       (rewrite (cryptol_ss ()) (unfold_term ["gcm_ghash_array_eq_property"] {{ gcm_ghash_array_eq_property`{blocks} }}));
   });

////////////////////////////////////////////////////////////////////////////////
// Specifications

/* == crypto/fipsmodule/modes/internal.h:272 ==
 * == generated-src/linux-x86_64/crypto/fipsmodule/ghash-x86_64.S ==
 *
 * void gcm_init_avx(u128 Htable[16], const uint64_t Xi[2]);
 */
let gcm_init_avx_spec = do {
  (_Htable, Htable_ptr) <- ptr_to_fresh "Htable" (llvm_array 12 (llvm_int 128));
  (Xi, Xi_ptr) <- ptr_to_fresh_readonly "Xi" (llvm_array 2 (llvm_int 64));

  crucible_execute_func [Htable_ptr, Xi_ptr];

  crucible_points_to Htable_ptr (crucible_term {{ gcm_init Xi }});
};

/* == crypto/fipsmodule/modes/internal.h:273 ==
 * == generated-src/linux-x86_64/crypto/fipsmodule/ghash-x86_64.S ==
 *
 * void gcm_gmult_avx(uint64_t Xi[2], const u128 Htable[16]);
 */
let gcm_gmult_avx_spec = do {
  (Xi, Xi_ptr) <- ptr_to_fresh "Xi" (llvm_array 16 (llvm_int 8));
  Htable_ptr <- crucible_alloc_readonly (llvm_array 12 (llvm_int 128));
  Htable0 <- crucible_fresh_var "Htable0" (llvm_int 128);
  crucible_points_to_untyped (crucible_elem Htable_ptr 1) (crucible_term {{ drop`{1} (gcm_init_Htable Htable0) }});
  crucible_points_to_untyped (crucible_elem Htable_ptr 0) (crucible_term {{ Htable0 }});

  crucible_execute_func [Xi_ptr, Htable_ptr];

  crucible_points_to Xi_ptr (crucible_term {{ gcm_gmult Htable0 Xi }});
};

/* == crypto/fipsmodule/modes/internal.h:274 ==
 * == generated-src/linux-x86_64/crypto/fipsmodule/ghash-x86_64.S ==
 *
 * void gcm_ghash_avx(uint64_t Xi[2], const u128 Htable[16], const uint8_t *in,
 *                    size_t len);
 */
let gcm_ghash_avx_concrete_spec blocks = do {
  let len = {{ `(blocks*AES_BLOCK_SIZE):[64] }};
  llvm_precond {{ len <= `GHASH_CHUNK }};

  (Xi, Xi_ptr) <- ptr_to_fresh "Xi" (llvm_array 16 (llvm_int 8));
  Htable0 <- crucible_fresh_var "Htable0" (llvm_int 128);
  Htable_ptr <- crucible_alloc_readonly (llvm_array 12 (llvm_int 128));

  crucible_points_to_untyped (crucible_elem Htable_ptr 0) (crucible_term {{ Htable0 }});
  crucible_points_to_untyped (crucible_elem Htable_ptr 1) (crucible_term {{ drop`{1} (gcm_init_Htable Htable0) }});

  (inp, inp_ptr) <- ptr_to_fresh_array_readonly "in" len;

  crucible_execute_func [Xi_ptr, Htable_ptr, inp_ptr, crucible_term len];

  crucible_points_to Xi_ptr (crucible_term {{ gcm_ghash_array Htable0 Xi inp 0 (`blocks:[64]) }});
};

let gcm_ghash_avx_spec = do {
  len <- llvm_fresh_var "len" (llvm_int 64);
  llvm_precond {{ len%16 == 0 }};
  llvm_precond {{ 0 < len }};
  llvm_precond {{ len < `(MAX_BLOCKS_AFTER_BULK*AES_BLOCK_SIZE) }};

  (Xi, Xi_ptr) <- ptr_to_fresh "Xi" (llvm_array 16 (llvm_int 8));
  Htable0 <- crucible_fresh_var "Htable0" (llvm_int 128);
  Htable_ptr <- crucible_alloc_readonly (llvm_array 12 (llvm_int 128));

  crucible_points_to_untyped (crucible_elem Htable_ptr 0) (crucible_term {{ Htable0 }});
  crucible_points_to_untyped (crucible_elem Htable_ptr 1) (crucible_term {{ drop`{1} (gcm_init_Htable Htable0) }});

  (inp, inp_ptr) <- ptr_to_fresh_array_readonly "in" len;

  crucible_execute_func [Xi_ptr, Htable_ptr, inp_ptr, crucible_term len];

  crucible_points_to Xi_ptr (crucible_term {{ gcm_ghash_array Htable0 Xi inp 0 (len/16) }});
};


////////////////////////////////////////////////////////////////////////////////
// Proof commands

enable_what4_hash_consing;

gcm_init_avx_ov <- llvm_verify_x86 m "../../build/x86/crypto/crypto_test" "gcm_init_avx"
  [ ("gcm_ghash_avx", 1800) // similar hack to the one in AESNI-GCM.saw to grab .L0x1c2_polynomial, we need a better way to handle this
  ]
  true
  gcm_init_avx_spec
  (do {
    unfolding ["gcm_init", "gcm_init_Htable"];
    simplify (addsimps [polyval_avx_thm] empty_ss);
    w4_unint_yices ["pmult"];
  });
disable_what4_hash_consing;


gcm_gmult_avx_ov <- llvm_verify_x86 m "../../build/x86/crypto/crypto_test" "gcm_gmult_avx"
  [ ("gcm_ghash_avx", 1800)
  ]
  true
  gcm_gmult_avx_spec
  rme;

enable_what4_hash_consing;

let gcm_ghash_avx_tactic = do {
  simplify (cryptol_ss ());
  simplify (addsimps gcm_ghash_array_thms empty_ss);
  simplify (addsimps gcm_ghash_avx_thms empty_ss);
  goal_eval_unint ["pmult", "gcm_polyval"];
  simplify (addsimps xor_slice_append_thms basic_ss);
  simplify (addsimps slice_slice_thms empty_ss);
  simplify (addsimps xor_slice_append_thms basic_ss);
  simplify (addsimps concat_assoc_0_thms empty_ss);
  simplify (addsimps concat_assoc_1_thms empty_ss);
  simplify (addsimps concat_assoc_2_thms empty_ss);
  w4_unint_z3 ["pmult", "gcm_polyval"];
};

gcm_ghash_avx_concrete_ovs <-
  for (eval_list {{ [1..MAX_BLOCKS_AFTER_BULK:[16]] }}) (\i -> do
    { let blocks = eval_int i;
      print (str_concat "gcm_ghash_avx concrete override: " (show blocks));
      llvm_verify_x86 m "../../build/x86/crypto/crypto_test" "gcm_ghash_avx"
        [ ("gcm_ghash_avx", 1800)
        ]
        true
        (gcm_ghash_avx_concrete_spec blocks)
        gcm_ghash_avx_tactic; });

disable_what4_hash_consing;

// TODO! implement and use specification refinement to turn the sequence
// of concrete specifications above into a single specification
gcm_ghash_avx_ov <-
  crucible_llvm_unsafe_assume_spec m "gcm_ghash_avx" gcm_ghash_avx_spec;
