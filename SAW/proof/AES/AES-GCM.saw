/*
 * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
 * SPDX-License-Identifier: Apache-2.0
*/

import "../../../cryptol-specs/Primitive/Symmetric/Cipher/Block/AES.cry";
import "../../../cryptol-specs/Primitive/Symmetric/Cipher/Authenticated/AES_256_GCM.cry";
import "../../spec/AES/X86.cry";
import "../../spec/AES/AES-GCM.cry";


enable_experimental;


// Disable debug intrinsics to avoid https://github.com/GaloisInc/crucible/issues/778
disable_debug_intrinsics;

m <- llvm_load_module "../../build/llvm/crypto/crypto_test.bc";

print "loaded bitcode";

include "../common/helpers.saw";
include "../common/memory.saw";

// Diasable proofs for now!
let do_prove = false;

include "goal-rewrites.saw";

/*
 * Architecture features for the AVX+shrd code path
 */
let {{ ia32cap = [0xffffffff, 0xffffffff, 0xffffffff, 0xffffffff] : [4][32] }};

interactive ();

include "AES.saw";

/*
 * Verification parameters.
 */

// TODO, find a more reasonable bound here
let TOTAL_MESSAGE_MAX_LENGTH = 10000;

// The total input length before the call to the update function.
let evp_cipher_update_gcm_len = 10;

// let continue = false;


/*
 * The GCM implementation has multiple phases:
 * 1) A "bulk" encryption/decryption that operates on multiples of 6 blocks, and computes
 *    AES/CTR32 and GHASH in parallel using different functional units of the CPU.
 * 2) An optimized AES/CTR32 implementation that processes the remaining blocks.
 * 3) An optimized GHASH implementation that processes the remaining blocks.
 *
 * The code below calculates the correct input lengths for each subroutine.
 */

// If starting auth length is nonzero, bytes are collected and a single GCM_MUL is performed.
// So bulk GCM auth length is the starting length rounded up to the next multiple of 16.
let aesni_gcm_cipher_gcm_len = eval_size {| (evp_cipher_update_gcm_len + 15) / 16 * 16 |};
let gcm_len_diff = eval_size {| aesni_gcm_cipher_gcm_len - evp_cipher_update_gcm_len |};
let aesni_gcm_cipher_len = eval_size {| evp_cipher_update_len - (min gcm_len_diff evp_cipher_update_len) |};

// To get size for bulk encrypt operatin, round down to nearest multiple of 96 (min 288)
let evp_cipher_update_bulk_encrypt = eval_size {| max 288 (aesni_gcm_cipher_len / 96 * 96) |};
let bulk_encrypt_used = eval_size {| aesni_gcm_cipher_len / evp_cipher_update_bulk_encrypt |};

// Separate AES-CTR32 and GHASH encryption on the remaining whole blocks
// When there are no blocks left after the bulk operation, prove the CTR32 and GHASH
// operations correct for 1 block. The code is not entered, so this proof is never used,
// but it avoids complications related to proving these operations correct on 0 blocks.
let length_after_bulk_encrypt = eval_size {| aesni_gcm_cipher_len - bulk_encrypt_used * evp_cipher_update_bulk_encrypt |};
let whole_blocks_after_bulk_encrypt = eval_size {| max 1 (length_after_bulk_encrypt / 16) |};
let GHASH_length_blocks_encrypt = eval_size {| whole_blocks_after_bulk_encrypt * 16|};

// Bulk decrypt core also operates on 6 blocks, but the minimum is 6 blocks.
let evp_cipher_update_bulk_decrypt = eval_size {| max 96 (aesni_gcm_cipher_len / 96 * 96) |};
let bulk_decrypt_used = eval_size {| aesni_gcm_cipher_len / evp_cipher_update_bulk_decrypt |};
// Separate AES-CTR32 and GHASH decryption on the remaining whole blocks
// Limit to minimum of 1 block like in encrypt case.
let length_after_bulk_decrypt = eval_size {| aesni_gcm_cipher_len - bulk_decrypt_used * evp_cipher_update_bulk_decrypt |};
let whole_blocks_after_bulk_decrypt = eval_size {| max 1 (length_after_bulk_decrypt / 16) |};
let GHASH_length_blocks_decrypt = eval_size {| whole_blocks_after_bulk_decrypt * 16|};

let evp_cipher_final_gcm_len = eval_size {| evp_cipher_update_gcm_len + evp_cipher_update_len |};

// Log the calculated values to help find errors when the proof fails.
print (str_concat "evp_cipher_update_len=" (show evp_cipher_update_len));
print (str_concat "aesni_gcm_cipher_gcm_len=" (show aesni_gcm_cipher_gcm_len));
print (str_concat "aesni_gcm_cipher_len=" (show aesni_gcm_cipher_len));

print (str_concat "evp_cipher_update_bulk_encrypt=" (show evp_cipher_update_bulk_encrypt));
print (str_concat "bulk_encrypt_used=" (show bulk_encrypt_used));
print (str_concat "length_after_bulk_encrypt=" (show length_after_bulk_encrypt));
print (str_concat "whole_blocks_after_bulk_encrypt=" (show whole_blocks_after_bulk_encrypt));
print (str_concat "GHASH_length_blocks_encrypt=" (show GHASH_length_blocks_encrypt));

print (str_concat "evp_cipher_update_bulk_decrypt=" (show evp_cipher_update_bulk_decrypt));
print (str_concat "bulk_decrypt_used=" (show bulk_decrypt_used));
print (str_concat "length_after_bulk_decrypt=" (show length_after_bulk_decrypt));
print (str_concat "whole_blocks_after_bulk_decrypt=" (show whole_blocks_after_bulk_decrypt));
print (str_concat "GHASH_length_blocks_decrypt=" (show GHASH_length_blocks_decrypt));

// if continue then return () else fail "Stop there!";

let NID_aes_256_gcm = 901;
let aes_block_size = 1;
// the IV for AES-GCM consists of 12 bytes = 96 bits
let aes_iv_len = 12;

include "GHASH.saw";

//interactive;
//callcc (\ (done : () -> TopLevel ()) -> subshell ());

include "AESNI-GCM.saw";

/*
When verifying aes_hw_ctr32_encrypt_blocks, the binary analysis must locally
treat r11 as callee-preserved. This is necessary because this routine saves
the original stack pointer in r11 and then calls helper routines, preventing
the binary analysis from inferring that the return address is still on the stack
when the routine returns. The called helper routines do not modify r11.
*/

let aes_hw_ctr32_tactic = do {
  simplify (cryptol_ss ());
  simplify (addsimps slice_384_thms basic_ss);
  simplify (addsimps [cmp_sub_thm] empty_ss);
  goal_eval_unint ["AESRound", "AESFinalRound", "aesenc", "aesenclast"];
  simplify (addsimps add_xor_slice_thms basic_ss);
  simplify (addsimps aesenclast_thms basic_ss);
  w4_unint_yices ["AESRound", "AESFinalRound"];
};

add_x86_preserved_reg "r11";
aes_hw_ctr32_encrypt_blocks_encrypt_ov <- llvm_verify_x86 m "../../build/x86/crypto/crypto_test" "aes_hw_ctr32_encrypt_blocks"
  []
  true
  aes_hw_ctr32_encrypt_blocks_spec
  aes_hw_ctr32_tactic;

// aes_hw_ctr32_encrypt_blocks_decrypt_ov <- llvm_verify_x86 m "../../build/x86/crypto/crypto_test" "aes_hw_ctr32_encrypt_blocks"
//   []
//   true
//   aes_hw_ctr32_encrypt_blocks_spec
//   aes_hw_ctr32_tactic;
default_x86_preserved_reg;



////////////////////////////////////////////////////////////////////////////////
// Specifications

let EVP_AES_GCM_CTX_PADDING = 8;
let EVP_AES_GCM_CTX_size = llvm_sizeof m (llvm_struct "struct.EVP_AES_GCM_CTX");
let ctx_size = eval_size {| EVP_AES_GCM_CTX_size + EVP_AES_GCM_CTX_PADDING |};


/*
 * Helpers for specifying the AES-GCM structs
 *
 * see == include/openssl/cipher.h:341-390 ==
 */
let EVP_CIPH_GCM_MODE = 0x6;
let EVP_CIPH_ALWAYS_CALL_INIT = 0x80;
let EVP_CIPH_CUSTOM_IV = 0x100;
let EVP_CIPH_CTRL_INIT = 0x200;
let EVP_CIPH_FLAG_CUSTOM_CIPHER = 0x400;
let EVP_CIPH_FLAG_AEAD_CIPHER = 0x800;
let EVP_CIPH_CUSTOM_COPY = 0x1000;

/* *** Data structure invariant for struct evp_cipher_st ***
 *
 * This data structure identifies a particular cipher, identifies
 * some parameters of the cipher, and contains function pointers
 * to the methods implementing it. This data structure is constant
 * throughout a particular cipher operation.  This spec fixes the
 * various parameters and flags for a particular AES-256 GCM operation,
 * which is the operation we verify here.
 *
 * == include/openssl/base.h:421 ==
 * typedef struct evp_cipher_st EVP_CIPHER;
 * 
 * == include/openssl/cipher.h:585 ==
 *
 * struct evp_cipher_st {
 *   // type contains a NID identifing the cipher. (e.g. NID_aes_128_gcm.)
 *   int nid;
 * 
 *   // block_size contains the block size, in bytes, of the cipher, or 1 for a
 *   // stream cipher.
 *   unsigned block_size;
 * 
 *   // key_len contains the key size, in bytes, for the cipher. If the cipher
 *   // takes a variable key size then this contains the default size.
 *   unsigned key_len;
 * 
 *   // iv_len contains the IV size, in bytes, or zero if inapplicable.
 *   unsigned iv_len;
 * 
 *   // ctx_size contains the size, in bytes, of the per-key context for this
 *   // cipher.
 *   unsigned ctx_size;
 * 
 *   // flags contains the OR of a number of flags. See |EVP_CIPH_*|.
 *   uint32_t flags;
 * 
 *   // app_data is a pointer to opaque, user data.
 *   void *app_data;
 * 
 *   int (*init)(EVP_CIPHER_CTX *ctx, const uint8_t *key, const uint8_t *iv,
 *               int enc);
 * 
 *   int (*cipher)(EVP_CIPHER_CTX *ctx, uint8_t *out, const uint8_t *in,
 *                 size_t inl);
 * 
 *   // cleanup, if non-NULL, releases memory associated with the context. It is
 *   // called if |EVP_CTRL_INIT| succeeds. Note that |init| may not have been
 *   // called at this point.
 *   void (*cleanup)(EVP_CIPHER_CTX *);
 * 
 *   int (*ctrl)(EVP_CIPHER_CTX *, int type, int arg, void *ptr);
 * };
 */
let points_to_evp_cipher_st ptr = do {
  crucible_points_to (crucible_elem ptr 0) (crucible_term {{ `NID_aes_256_gcm : [32] }});
  crucible_points_to (crucible_elem ptr 1) (crucible_term {{ `aes_block_size : [32] }});
  crucible_points_to (crucible_elem ptr 2) (crucible_term {{ `aes_key_len : [32] }});
  crucible_points_to (crucible_elem ptr 3) (crucible_term {{ `aes_iv_len : [32] }});
  crucible_points_to (crucible_elem ptr 4) (crucible_term {{ `ctx_size : [32] }});

  let flags = eval_size {| EVP_CIPH_GCM_MODE + EVP_CIPH_CUSTOM_IV + EVP_CIPH_CUSTOM_COPY +
                           EVP_CIPH_FLAG_CUSTOM_CIPHER + EVP_CIPH_ALWAYS_CALL_INIT +
                           EVP_CIPH_CTRL_INIT + EVP_CIPH_FLAG_AEAD_CIPHER |};
  crucible_points_to (crucible_elem ptr 5) (crucible_term {{ `flags : [32] }});

  crucible_points_to (crucible_elem ptr 6) crucible_null;
  crucible_points_to (crucible_elem ptr 7) (crucible_global "aes_gcm_init_key");
  crucible_points_to (crucible_elem ptr 8) (crucible_global "aes_gcm_cipher");
  crucible_points_to (crucible_elem ptr 9) (crucible_global "aes_gcm_cleanup");
  crucible_points_to (crucible_elem ptr 10) (crucible_global "aes_gcm_ctrl");
};

/* == include/openssl/base.h:420 ==
 * typedef struct evp_cipher_ctx_st EVP_CIPHER_CTX;
 * 
 * == include/openssl/cipher.h:536 ==
 *
 * struct evp_cipher_ctx_st {
 *   // cipher contains the underlying cipher for this context.
 *   const EVP_CIPHER *cipher;
 * 
 *   // app_data is a pointer to opaque, user data.
 *   void *app_data;      // application stuff
 * 
 *   // cipher_data points to the |cipher| specific state.
 *   void *cipher_data;
 * 
 *   // key_len contains the length of the key, which may differ from
 *   // |cipher->key_len| if the cipher can take a variable key length.
 *   unsigned key_len;
 * 
 *   // encrypt is one if encrypting and zero if decrypting.
 *   int encrypt;
 * 
 *   // flags contains the OR of zero or more |EVP_CIPH_*| flags, above.
 *   uint32_t flags;
 * 
 *   // oiv contains the original IV value.
 *   uint8_t oiv[EVP_MAX_IV_LENGTH];
 * 
 *   // iv contains the current IV value, which may have been updated.
 *   uint8_t iv[EVP_MAX_IV_LENGTH];
 * 
 *   // buf contains a partial block which is used by, for example, CTR mode to
 *   // store unused keystream bytes.
 *   uint8_t buf[EVP_MAX_BLOCK_LENGTH];
 * 
 *   // buf_len contains the number of bytes of a partial block contained in
 *   // |buf|.
 *   int buf_len;
 * 
 *   // num contains the number of bytes of |iv| which are valid for modes that
 *   // manage partial blocks themselves.
 *   unsigned num;
 * 
 *   // final_used is non-zero if the |final| buffer contains plaintext.
 *   int final_used;
 * 
 *   uint8_t final[EVP_MAX_BLOCK_LENGTH];  // possible final block
 * } /* EVP_CIPHER_CTX */;
 */
let points_to_evp_cipher_ctx_st ptr cipher_ptr cipher_data_ptr enc = do {
  crucible_points_to (crucible_field ptr "cipher") cipher_ptr;
  crucible_points_to (crucible_field ptr "cipher_data") cipher_data_ptr;
  crucible_points_to (crucible_field ptr "key_len") (crucible_term {{ `aes_key_len : [32] }});
  crucible_points_to (crucible_field ptr "encrypt") (crucible_term enc);
  crucible_points_to (crucible_field ptr "flags") (crucible_term {{ 0 : [32] }});
  crucible_points_to (crucible_field ptr "buf_len") (crucible_term {{ 0 : [32] }});
  crucible_points_to (crucible_field ptr "final_used") (crucible_term {{ 0 : [32] }});
};

/* == cryptol-specs/Primitive/Symmetric/Cipher/Authenticated/AES_256_GCM.cry ==
 *
 * type AES_GCM_Ctx =
 *  { key : [32][8]
 *  , iv : [12][8]
 *  , Xi : [16][8]
 *  , len : [64]
 *  }
 */
let fresh_aes_gcm_ctx = do {
  key <- fresh_aes_key_st;
  iv <- crucible_fresh_var "iv" (llvm_array aes_iv_len (llvm_int 8));
  Xi <- crucible_fresh_var "Xi" (llvm_array AES_BLOCK_SIZE (llvm_int 8));

  gcm_len <- crucible_fresh_var "gcm_len" (llvm_int 64);
  return {{ { key = key, iv = iv, Xi = Xi, len = gcm_len } : AES_GCM_Ctx }};
};


/* This data structure contains some precomputed data related to the
 * encrypt/decrypt key, and contains function pointers that implement
 * the blockwise sub-operations.
 *
 * == crypto/fipsmodule/modes/internal.h:129 ==
 *
 * typedef struct gcm128_key_st {
 *   // Note the MOVBE-based, x86-64, GHASH assembly requires |H| and |Htable| to
 *   // be the first two elements of this struct. Additionally, some assembly
 *   // routines require a 16-byte-aligned |Htable| when hashing data, but not
 *   // initialization. |GCM128_KEY| is not itself aligned to simplify embedding in
 *   // |EVP_AEAD_CTX|, but |Htable|'s offset must be a multiple of 16.
 *   u128 H;
 *   u128 Htable[16];
 *   gmult_func gmult;
 *   ghash_func ghash;
 * 
 *   block128_f block;
 * 
 *   // use_aesni_gcm_crypt is true if this context should use the assembly
 *   // functions |aesni_gcm_encrypt| and |aesni_gcm_decrypt| to process data.
 *   unsigned use_aesni_gcm_crypt:1;
 * } GCM128_KEY;
 */
let points_to_gcm128_key_st ptr ctx = do {
  crucible_points_to_untyped (crucible_elem ptr 0) (crucible_term {{ get_H ctx }});
  crucible_points_to_untyped (crucible_elem ptr 1) (crucible_term {{ get_Htable ctx }});
  crucible_points_to (crucible_elem ptr 2) (crucible_global "gcm_gmult_avx");
  crucible_points_to (crucible_elem ptr 3) (crucible_global "gcm_ghash_avx");
  crucible_points_to (crucible_elem ptr 4) (crucible_global "aes_hw_encrypt");
  crucible_points_to (crucible_elem ptr 5) (crucible_term {{ 1 : [8] }});
};

/* == crypto/fipsmodule/modes/internal.h:147 ==
 *
 * // GCM128_CONTEXT contains state for a single GCM operation. The structure
 * // should be zero-initialized before use.
 * typedef struct {
 *   // The following 5 names follow names in GCM specification
 *   union {
 *     uint64_t u[2];
 *     uint32_t d[4];
 *     uint8_t c[16];
 *     crypto_word_t t[16 / sizeof(crypto_word_t)];
 *   } Yi, EKi, EK0, len, Xi;
 * 
 *   // Note that the order of |Xi| and |gcm_key| is fixed by the MOVBE-based,
 *   // x86-64, GHASH assembly. Additionally, some assembly routines require
 *   // |gcm_key| to be 16-byte aligned. |GCM128_KEY| is not itself aligned to
 *   // simplify embedding in |EVP_AEAD_CTX|.
 *   alignas(16) GCM128_KEY gcm_key;
 * 
 *   unsigned mres, ares;
 * } GCM128_CONTEXT;
 * 
 */
let points_to_GCM128_CONTEXT ptr ctx = do {
  // let {{ mres = drop`{32} (ctx.len % `AES_BLOCK_SIZE) }};
  // TODO lift this restriction
  let {{ mres = 0:[32] }};

  crucible_points_to_untyped (crucible_elem ptr 0) (crucible_term {{ get_Yi ctx }});
  llvm_points_to_untyped (crucible_elem ptr 1) (crucible_term {{ get_EKi ctx }});
  crucible_points_to_untyped (crucible_elem ptr 2) (crucible_term {{ get_EK0 ctx }});
  crucible_points_to_untyped (crucible_elem ptr 3) (crucible_term {{ [(0 : [64]), ctx.len] }});
  crucible_points_to_untyped (crucible_elem ptr 4) (crucible_term {{ ctx.Xi }});
  points_to_gcm128_key_st (crucible_elem ptr 5) ctx;
  crucible_points_to (crucible_elem ptr 6) (crucible_term {{ mres : [32] }});
  crucible_points_to (crucible_elem ptr 7) (crucible_term {{ 0 : [32] }});
};

/* == crypto/fipsmodule/cipher/e_aes.c ==
 *
 * typedef struct {
 *   GCM128_CONTEXT gcm;
 *   union {
 *     double align;
 *     AES_KEY ks;
 *   } ks;         // AES key schedule to use
 *   int key_set;  // Set if key initialised
 *   int iv_set;   // Set if an iv is set
 *   uint8_t *iv;  // Temporary IV store
 *   int ivlen;         // IV length
 *   int taglen;
 *   int iv_gen;      // It is OK to generate IVs
 *   ctr128_f ctr;
 * } EVP_AES_GCM_CTX;
 *
 */
let points_to_EVP_AES_GCM_CTX ptr ctx iv_set taglen = do {
  points_to_GCM128_CONTEXT (crucible_field ptr "gcm") ctx;
  points_to_aes_key_st (crucible_field ptr "ks") {{ ctx.key }};
  crucible_points_to (crucible_field ptr "key_set") (crucible_term {{ 1 : [32] }});
  crucible_points_to (crucible_field ptr "iv_set") (crucible_term iv_set);
  crucible_points_to (crucible_field ptr "ivlen") (crucible_term {{ `aes_iv_len : [32] }});
  crucible_points_to (crucible_field ptr "taglen") (crucible_term {{ `taglen : [32] }});
  crucible_points_to (crucible_field ptr "iv_gen") (crucible_term {{ 0 : [32] }});
  crucible_points_to (crucible_field ptr "ctr") (crucible_global "aes_hw_ctr32_encrypt_blocks");
};


/* == crypto/fipsmodule/cipher/e_aes.c ==
 *
 * static EVP_AES_GCM_CTX *aes_gcm_from_cipher_ctx(EVP_CIPHER_CTX *ctx) { ... }
 */
let aes_gcm_from_cipher_ctx_spec = do {
  cipher_data_ptr <- crucible_alloc_readonly_aligned 16 (llvm_struct "struct.EVP_AES_GCM_CTX");

  crucible_execute_func [cipher_data_ptr];

  crucible_return cipher_data_ptr;
};

let evp_cipher_tactic = do {
  unfolding ["cipher_update", "cipher_update_byte", "cipher_final"];
  simplify (addsimp gcm_pmult_pmod_thm empty_ss);
  w4_unint_yices ["pmult", "pmod", "gcm_polyval", "aes_hw_encrypt"];
};

include "evp-function-specs.saw";

////////////////////////////////////////////////////////////////////////////////
// Proof commands

aes_gcm_from_cipher_ctx_ov <- crucible_llvm_unsafe_assume_spec
  m
  "aes_gcm_from_cipher_ctx"
  aes_gcm_from_cipher_ctx_spec;


llvm_verify m "aes_256_gcm_generic_init" [] true aes_256_gcm_generic_init_spec (w4_unint_yices []);


// let's actually do proofs again...
let do_prove = true;

let evp_cipher_ovs =
  [ OPENSSL_malloc_ov
  , aes_gcm_from_cipher_ctx_ov
  , aes_hw_set_encrypt_key_ov
  , aes_hw_encrypt_ov
  , aes_hw_encrypt_in_place_ov
  , aes_hw_ctr32_encrypt_blocks_encrypt_ov
//  , aes_hw_ctr32_encrypt_blocks_decrypt_ov
  , gcm_init_avx_ov
  , gcm_gmult_avx_ov
  , gcm_ghash_avx_encrypt_ov
//  , gcm_ghash_avx_decrypt_ov
  , aesni_gcm_encrypt_ov
  , aesni_gcm_decrypt_ov
  ];


// llvm_verify m "EVP_CipherInit_ex"
//   evp_cipher_ovs
//   true
//   (EVP_CipherInit_ex_spec {{ 1 : [32] }})
//   evp_cipher_tactic;

// llvm_verify m "EVP_CipherInit_ex"
//   evp_cipher_ovs
//   true
//   (EVP_CipherInit_ex_spec {{ 0 : [32] }})
//   evp_cipher_tactic;


enable_what4_hash_consing;

llvm_verify m "EVP_EncryptUpdate"
  evp_cipher_ovs
  true
  (EVP_CipherUpdate_spec {{ 1 : [32] }})
  evp_cipher_tactic;

// llvm_verify m "EVP_DecryptUpdate"
//   evp_cipher_ovs
//   true
//   (EVP_CipherUpdate_spec {{ 0 : [32] }} evp_cipher_update_gcm_len evp_cipher_update_len)
//   evp_cipher_tactic;

disable_what4_hash_consing;


// llvm_verify m "EVP_EncryptFinal_ex"
//   evp_cipher_ovs
//   true
//   EVP_EncryptFinal_ex_spec
//   evp_cipher_tactic;

// llvm_verify m "EVP_DecryptFinal_ex"
//   evp_cipher_ovs
//   true
//   EVP_DecryptFinal_ex_spec
//   evp_cipher_tactic;

