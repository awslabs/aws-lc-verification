
/*
 *
 * void aes_hw_ctr32_encrypt_blocks(const uint8_t *in, uint8_t *out, size_t len,
 *                                  const AES_KEY *key, const uint8_t ivec[16]);
 *
 * NOTE! len here is the number of _BLOCKS_
 *
 * This is currently a bounded spec, as we will handle fewer than
 * MAX_BLOCKS_AFTER_BULK blocks. It may be possible to lift this
 * restriction by competing the partial unbounded proof below.
 */
let aes_hw_ctr32_encrypt_blocks_spec = do {
  global_alloc_init "OPENSSL_ia32cap_P" {{ ia32cap }};

  blocks <- llvm_fresh_var "blocks" (llvm_int 64);

  // We only need to prove this operation correct for at most
  // 18 blocks, as the bulk encryption phase will pass through
  // at most that many, and the decryption phase fewer.
  llvm_precond {{ 0 < blocks /\ blocks < `MAX_BLOCKS_AFTER_BULK }};

  let len = {{ blocks * `AES_BLOCK_SIZE }};

  (in_, in_ptr) <- ptr_to_fresh_array_readonly "in" len;
  (out_, out_ptr) <- ptr_to_fresh_array "out" len;
  key_ptr <- crucible_alloc_readonly (llvm_struct "struct.aes_key_st");
  key <- fresh_aes_key_st;
  points_to_aes_key_st key_ptr key;
  (ivec, ivec_ptr) <- ptr_to_fresh_readonly "ivec" (llvm_array AES_BLOCK_SIZE (llvm_int 8));

  crucible_execute_func [in_ptr, out_ptr, crucible_term blocks, key_ptr, ivec_ptr];

  let out_data = {{ aes_hw_ctr32_encrypt_blocks_array in_ key ivec 0 blocks out_ }};
  llvm_setup_with_tag "output buffer"
    (crucible_points_to_array_prefix out_ptr out_data len);

  global_alloc_init "OPENSSL_ia32cap_P" {{ ia32cap }};
};

// This version of the spec takes a concrete number of blocks
// as input.
let aes_hw_ctr32_encrypt_blocks_concrete_spec blocks = do {
  global_alloc_init "OPENSSL_ia32cap_P" {{ ia32cap }};

  let len = {{ `(blocks * AES_BLOCK_SIZE):[64] }};

  (in_, in_ptr) <- ptr_to_fresh_array_readonly "in" len;
  (out_, out_ptr) <- ptr_to_fresh_array "out" len;
  key_ptr <- crucible_alloc_readonly (llvm_struct "struct.aes_key_st");
  key <- fresh_aes_key_st;
  points_to_aes_key_st key_ptr key;
  (ivec, ivec_ptr) <- ptr_to_fresh_readonly "ivec" (llvm_array AES_BLOCK_SIZE (llvm_int 8));

  crucible_execute_func [in_ptr, out_ptr, crucible_term {{ `blocks:[64] }}, key_ptr, ivec_ptr];

  let out_data = {{ aes_hw_ctr32_encrypt_blocks_array in_ key ivec 0 `blocks out_ }};
  llvm_setup_with_tag "output buffer postcondition"
    (crucible_points_to_array_prefix out_ptr out_data {{ len }});

  global_alloc_init "OPENSSL_ia32cap_P" {{ ia32cap }};
};


// The following collection of intermeidate lemmas are useful to
// stitch together the individual bounded proofs into the overall
// proof we want.
aes_hw_ctr32_eq_thms <-
  for (eval_list {{ [ 1:[16] .. < MAX_BLOCKS_AFTER_BULK ] }}) (\i ->
    do { let blocks = eval_int i;
         print (str_concat "aes_hw_ctr32 eq lemma: " (show blocks));
         prove_theorem
           do { goal_normalize ["aes_hw_encrypt"]; trivial; }
           (rewrite (cryptol_ss ())
            (unfold_term ["aes_hw_ctr32_eq_property"]
             (term_apply {{ aes_hw_ctr32_eq_property`{blocks=blocks} }}
	                 [ parse_core "arrayEq (Vec 64 Bool) (Vec 8 Bool)" ]
             )));
       });

aes_hw_ctr32_copy_thms <-
  for (eval_list {{ [ 1:[16] .. < MAX_BLOCKS_AFTER_BULK ] }}) (\i ->
    do { let blocks = eval_int i;
         print (str_concat "aes_hw_ctr32 copy lemma: " (show blocks));
         prove_theorem
           do { w4_unint_z3 ["aes_hw_encrypt"]; }
           (rewrite (cryptol_ss ())
            (unfold_term ["aes_hw_ctr32_copy_property"]
             (term_apply {{ aes_hw_ctr32_copy_property`{blocks=blocks} }}
	                 [ parse_core "arrayEq (Vec 64 Bool) (Vec 8 Bool)" ]
             )));
       });


// This is the original spec for the AES/CTR32 mode routine
// which operates on standard Cryptol sequences instead
// of "Array" values.
let original_aes_hw_ctr32_encrypt_blocks_spec blocks = do {
  let len = eval_size {| blocks * AES_BLOCK_SIZE |};
  global_alloc_init "OPENSSL_ia32cap_P" {{ ia32cap }};

  (in_, in_ptr) <- ptr_to_fresh_readonly "in" (llvm_array len (llvm_int 8));
  out_ptr <- crucible_alloc (llvm_array len (llvm_int 8));
  key_ptr <- crucible_alloc_readonly (llvm_struct "struct.aes_key_st");
  key <- fresh_aes_key_st;
  points_to_aes_key_st key_ptr key;
  (ivec, ivec_ptr) <- ptr_to_fresh_readonly "ivec" (llvm_array AES_BLOCK_SIZE (llvm_int 8));

  crucible_execute_func [in_ptr, out_ptr, (crucible_term {{ `blocks : [64] }}), key_ptr, ivec_ptr];

  crucible_points_to out_ptr (crucible_term {{ aes_hw_ctr32_encrypt_blocks in_ key ivec }});

  global_alloc_init "OPENSSL_ia32cap_P" {{ ia32cap }};
};

// The following commented-out section is the beginning of an unbounded
// proof for the AES/CTR32 mode routine. It is currently complete enough
// to prove memory safety and establish some of the necesary bookkeeping
// properties. To complete it, one needs to establish the basic invariant
// property regarding the computation of the output array (the one
// listed below is probably in the right neighborhood), and then use it
// to establish the final postcondition. I don't expect this to be fundamentally
// difficult (and is probably easier than the bulk encryption proofs). However,
// there are 8 blocks in flight at a time in the loop body, and the post-loop
// code has a pretty gnarly sequence of tests to compute the final at-most-seven
// blocks, so the terms involved get quite large.

// let {{
//   bswap4 : [32] -> [32]
//   bswap4 x = join (reverse ((split x):[4][8]))

//   type aes_hw_ctr32_invariant_tuple =
//     ( Array [64] [8]
//     , [32], [32]
//     , [512], [512], [512], [512], [512], [512], [512], [512]
//     , [512], [512], [512], [512], [512], [512], [512], [512]
//     , [64], [64], [64], [64]
//     )

//   aes_hw_ctr32_encrypt_blocks_invariant :
//     Array [64] [8] ->
//     [32][8] ->
//     [16][8] ->
//     aes_hw_ctr32_invariant_tuple ->
//     aes_hw_ctr32_invariant_tuple ->
//     Bool
//   aes_hw_ctr32_encrypt_blocks_invariant in key ivec
//     ( i_buf, _, _
//     , _, _, _, _, _, _, _, _
//     , _, _, _, _, _, _, _, _
//     , _, _, _, i_blocks )

//     ( c_buf
//     , c_temp1, c_temp2

//     , _, _, _, _, _, _, _, _ // junk values?

//     , xmm7, xmm6, xmm5, xmm4, xmm3, xmm2, xmm1, xmm0

//     , c_cnt, c_off1, c_off2 , c_blocks )

//     =  i_blocks >= c_blocks
//     /\ processed_blocks%8 == 0
//     /\ c_off1 == offset
//     /\ c_off2 == offset
//     /\ c_cnt  == zext cnt

//     /\ c_temp1 == cnt_and_key 6
//     /\ c_temp2 == cnt_and_key 7

//     // xmm0 and xmm1 seem to just be scratch registers at this point?

//     /\ drop`{512-128} xmm2 == ivec_and_key 0
//     /\ drop`{512-128} xmm3 == ivec_and_key 1
//     /\ drop`{512-128} xmm4 == ivec_and_key 2
//     /\ drop`{512-128} xmm5 == ivec_and_key 3
//     /\ drop`{512-128} xmm6 == ivec_and_key 4
//     /\ drop`{512-128} xmm7 == ivec_and_key 5

// // Now to actually reason about the encryption...
// //    /\ arrayEq (aes_hw_ctr32_encrypt_blocks_array in key ivec 0 i_blocks i_buf)
// //               (aes_hw_ctr32_encrypt_blocks_array in key ivec processed_blocks i_blocks c_buf)

//     where
//       top_ivec_and_key : [96]
//       top_ivec_and_key = join (reverse (take`{12} ivec)) ^ join (reverse (take`{12} key))

//       cnt_and_key : [32] -> [32]
//       cnt_and_key i = (bswap4 (cnt + i)) ^ (join (reverse (take`{4} (drop`{12} key))))

//       ivec_and_key : [32] -> [128]
//       ivec_and_key i = cnt_and_key i # top_ivec_and_key

//       processed_blocks = i_blocks - c_blocks
//       offset = processed_blocks * 16
//       i_cnt = join (drop`{12} ivec)
//       cnt = i_cnt + drop`{32} processed_blocks

// }};

// let go = w4_unint_yices
//   [ "aes_hw_ctr32_encrypt_blocks_array","ExpandKey","aesenc","aesenclast"
//   , "AESRound", "AESFinalRound"
//   ];

// add_x86_preserved_reg "r11";

// unbounded_aes_hw_ctr32_ovr <- llvm_verify_x86_with_invariant
//   m "../../build/x86/crypto/crypto_test" "aes_hw_ctr32_encrypt_blocks"
//     []
//     true
//     ("aes_hw_ctr32_encrypt_blocks", 2, {{ aes_hw_ctr32_encrypt_blocks_invariant }})
//     aes_hw_ctr32_encrypt_blocks_spec
//     do {
//       loop_inv0 <- goal_has_some_tag ["initial loop invariant"];
//       loop_inv  <- goal_has_some_tag ["inductive loop invariant"];
//       out_goal  <- goal_has_some_tag ["output buffer"];
//       if loop_inv0 then do {
//         return (run (print "<<initial loop invariant>>"));
// 	go;
//       } else if loop_inv then do {
//         return (run (print "<<inductive loop invariant>>"));
// 	go;
//       } else if out_goal then do {
//         return (run (print "<<output buffer goal>>"));
// 	admit "TODO";
//       } else do {
//          print_goal_summary;
// 	 go;
//       };
//     };

// default_x86_preserved_reg;


let aes_hw_ctr32_tactic = do {
  print_goal_summary;
  simplify (cryptol_ss ());
  simplify (addsimps slice_384_thms basic_ss);
  simplify (addsimps [cmp_sub_thm] empty_ss);
  goal_eval_unint ["AESRound", "AESFinalRound", "aesenc", "aesenclast"];
  simplify (addsimps add_xor_slice_thms basic_ss);
  simplify (addsimps aesenclast_thms basic_ss);
  w4_unint_yices ["AESRound", "AESFinalRound"];
};

/*
When verifying aes_hw_ctr32_encrypt_blocks, the binary analysis must locally
treat r11 as callee-preserved. This is necessary because this routine saves
the original stack pointer in r11 and then calls helper routines, preventing
the binary analysis from inferring that the return address is still on the stack
when the routine returns. The called helper routines do not modify r11.
*/

add_x86_preserved_reg "r11";

// Prove the specification on Arrays for each of the concrete sizes
// we need. This is done by first proving the spec for standard
// Cryptol sequences, and using refinement to demonstrate the spec
// for Arrays.
//
// TODO: it may be more memory-efficent to instead
// to the proof directly on the Array-based specification.

aes_hw_ctr32_encrypt_blocks_concrete_ovs <-
  for (eval_list {{ [ 1:[16] .. < MAX_BLOCKS_AFTER_BULK ] }}) (\i -> do
  { let blocks = eval_int i;
    print (str_concat "aes_hw_ctr32_encrypt lemma: " (show blocks));
//    if do_prove then do {
    // TODO! This is currently limited to only do the concrete-size proofs
    // up to 5 blocks in order to control memory useage.
    if eval_bool {{ i < 6 }} then do {
      ov <- llvm_verify_x86 m "../../build/x86/crypto/crypto_test" "aes_hw_ctr32_encrypt_blocks"
        []
        true
        (original_aes_hw_ctr32_encrypt_blocks_spec blocks)
        aes_hw_ctr32_tactic;
      llvm_refine_spec m "aes_hw_ctr32_encrypt_blocks" [ov]
        (aes_hw_ctr32_encrypt_blocks_concrete_spec blocks)
        do { simplify (addsimps aes_hw_ctr32_eq_thms (cryptol_ss ()));
      	     w4_unint_z3 ["aes_hw_ctr32_encrypt_blocks"];
    	   };
    } else
      crucible_llvm_unsafe_assume_spec m "aes_hw_ctr32_encrypt_blocks"
        (aes_hw_ctr32_encrypt_blocks_concrete_spec blocks);
 });

// restore the default state of preserved registers
default_x86_preserved_reg;

// Now we take all the individual specifications at concrete sizes
// and combine them into a proof of the overall desired specification.
aes_hw_ctr32_encrypt_blocks_ov <-
  llvm_refine_spec m "aes_hw_ctr32_encrypt_blocks"
    aes_hw_ctr32_encrypt_blocks_concrete_ovs
    aes_hw_ctr32_encrypt_blocks_spec
    do {
      hard_goal <- goal_has_some_tag ["output buffer"];
      if hard_goal then do {
        print_goal_summary;
  	simplify (addsimps aes_hw_ctr32_copy_thms (cryptol_ss ()));
  	w4_unint_z3 ["aes_hw_ctr32_encrypt_blocks_array"];
      } else do {
        yices;
      };
    };
