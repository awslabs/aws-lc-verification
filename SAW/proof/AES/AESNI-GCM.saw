/*
 * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
 * SPDX-License-Identifier: Apache-2.0
 */

// TODO, move this to a better place

// The GCM counter starts at 1 and we need to leave a block at the end
// for the authentication tag. This gives us a total of slightly fewer
// than 2^^32 blocks we can handle.
let TOTAL_MESSAGE_BLOCKS = eval_size {| 2^^32 - 2 |};

// This is the total number of bytes that can be in the plain/cyphertext
// for AES-GCM.
let TOTAL_MESSAGE_MAX_LENGTH = eval_size {| TOTAL_MESSAGE_BLOCKS * AES_BLOCK_SIZE |};

////////////////////////////////////////////////////////////////////////////////
// Rewrite rules and tactics specific to the AESNI-GCM proofs

// TODO! make this independent of the length!
let prove_ctr32_encrypt_thm gcm_len len = prove_theorem
  (do {
    w4_unint_yices ["aes_hw_encrypt"];
  })
  (rewrite (cryptol_ss ()) {{ \key iv Xi (in : [len][8]) -> ctr32_encrypt ({ key = key, iv = iv, Xi = Xi, len = `gcm_len } : AES_GCM_Ctx) in == join (map split (aesni_gcm_ctr32_encrypt_block ({ key = key, iv = iv, Xi = Xi, len = `gcm_len } : AES_GCM_Ctx) in)) }});

// TODO! reenable these proofs
let ctr32_encrypt_thms = [];

// ctr32_encrypt_encrypt_thm <- prove_ctr32_encrypt_thm aesni_gcm_cipher_gcm_len evp_cipher_update_bulk_encrypt;
// ctr32_encrypt_decrypt_thm <- prove_ctr32_encrypt_thm aesni_gcm_cipher_gcm_len evp_cipher_update_bulk_decrypt;
// let ctr32_encrypt_thms =
//   [ ctr32_encrypt_encrypt_thm
//   , ctr32_encrypt_decrypt_thm
//   ];

// TODO, generalize lengths...
let prove_cipher_update_avx_thm enc gcm_len len = prove_theorem
  (do {
    unfolding ["cipher_update", "cipher_update_byte"];
    simplify (cryptol_ss ());
    simplify (addsimps ctr32_encrypt_thms empty_ss);
    simplify (addsimp gcm_pmult_pmod_thm empty_ss);
    goal_eval_unint ["pmult", "pmod", "gcm_polyval", "gcm_polyval_mul", "gcm_polyval_mul_pmult3", "gcm_polyval_mul_pmult4", "gcm_polyval_red", "gcm_polyval_red_pmult", "aes_hw_encrypt", "aesni_gcm_ctr32_encrypt_block"];
    simplify (addsimps gcm_polyval_thms empty_ss);
    simplify (addsimps [concat_assoc_0_thm] empty_ss);
    w4_unint_yices ["pmult", "pmod", "gcm_polyval", "aes_hw_encrypt"];
  })
  (rewrite (cryptol_ss ()) {{ \key iv Xi (in : [len][8]) -> (cipher_update enc ({ key = key, iv = iv, Xi = Xi, len = `gcm_len } : AES_GCM_Ctx) in).Xi == aesni_gcm_cipher enc ({ key = key, iv = iv, Xi = Xi, len = `gcm_len } : AES_GCM_Ctx) in }});


// TODO Reenable these proofs!
let cipher_update_avx_thms = [];

// encrypt_update_avx_thm <- prove_cipher_update_avx_thm {{ 1 : [32] }} aesni_gcm_cipher_gcm_len evp_cipher_update_bulk_encrypt;
// decrypt_update_avx_thm <- prove_cipher_update_avx_thm {{ 0 : [32] }} aesni_gcm_cipher_gcm_len evp_cipher_update_bulk_decrypt;
// let cipher_update_avx_thms =
//   [ encrypt_update_avx_thm
//   , decrypt_update_avx_thm
//   ];


////////////////////////////////////////////////////////////////////////////////
// Specifications


/* The spec for the "bulk encryption" phase.
 *
 * == crypto/fipsmodule/modes/internal.h:277 ==
 * == generated-src/linux-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.S ==
 *
 * size_t aesni_gcm_encrypt(const uint8_t *in, uint8_t *out, size_t len,
 *                          const AES_KEY *key, uint8_t ivec[16], uint64_t *Xi);
 * size_t aesni_gcm_decrypt(const uint8_t *in, uint8_t *out, size_t len,
 *                          const AES_KEY *key, uint8_t ivec[16], uint64_t *Xi);
 * 
 * The enc parameter determines if this is the encryption or decryption specification.
 * enc = 1 means encryption; enc = 0 means decryption.
 */
let aesni_gcm_cipher_spec (enc:Term) : LLVMSetup () =
  llvm_setup_with_tag "AESNI GCM cipher spec"
  do {

  // the "len" value is a signed 32-bit value upcasted to
  // an unsigned 64-bit value. Here we assert that the value
  // fits into the positive portion of a signed 32-bit integer.
  len64 <- llvm_fresh_var "len64" (llvm_int 64);
  llvm_precond {{ len64 < 2^^31 }};

  let encT = eval_int {{ enc }};

  // Bulk encrypt requires 3*6 blocks of input. Bulk decrypt only requires 6 blocks.
  let min_size = eval_size {| (1 + 2 * encT) * 6 * AES_BLOCK_SIZE |};

  // AES key structure, containing the expanded key
  key <- fresh_aes_key_st;
  key_ptr <- crucible_alloc_readonly (llvm_struct "struct.aes_key_st");
  points_to_aes_key_st key_ptr key;

  // IVec buffer, consisting of the IV and the block counter
  ivec_ptr <- crucible_alloc (llvm_array AES_BLOCK_SIZE (llvm_int 8));
  ivec_bytes <- crucible_fresh_var "ivec" (llvm_array AES_BLOCK_SIZE (llvm_int 8));
  crucible_points_to ivec_ptr (crucible_term ivec_bytes);

  Xi <- crucible_fresh_var "Xi" (llvm_array AES_BLOCK_SIZE (llvm_int 8));

  let {{ // NB, big-endian counter
         ctr = join (drop`{12} ivec_bytes)
	 iv  = take`{12} ivec_bytes

	 // NB, the GCM counter runs ahead
	 gcm_len = ((zext (ctr-2)):[64]) * `AES_BLOCK_SIZE

         ctx = { key = key, iv = iv , Xi = Xi, len = gcm_len } : AES_GCM_Ctx
 
         // Compute how much data we expect the bulk en/decryption operation to process.
         len32 = drop`{32} len64
         bulk_len = max `min_size ((len32 / `(6 * AES_BLOCK_SIZE)) * `(6 * AES_BLOCK_SIZE))
         do_bulk  = len32 / bulk_len
	 res_len  = do_bulk * bulk_len
	 res_len64 = (zext res_len):[64]
	 res_ctr  = ctr + (res_len / `AES_BLOCK_SIZE) }};

  // Input plaintext bytes, length "len"
  (in_, in_ptr) <- ptr_to_fresh_array_readonly "in" {{ res_len64 }};

  // Output ciphertext bytes, length "len"
  (out_, out_ptr) <- ptr_to_fresh_array "out" {{ res_len64 }};

  llvm_setup_with_tag "size constraints" do {
    llvm_precond {{ 1 < ctr }};
    llvm_precond {{ gcm_len <= `TOTAL_MESSAGE_MAX_LENGTH }};
    llvm_precond {{ gcm_len + len64 <= `TOTAL_MESSAGE_MAX_LENGTH }};
  };

  // The Xi pointer, consisting of the actual current Xi value,
  // the Hashing key "H" and the auxilary Htable value.  H and
  // Htable are constant for the duration of the en/decryption operation.
  Xi_ptr <- crucible_alloc (llvm_array 14 (llvm_int 128));

  crucible_points_to_untyped (crucible_elem Xi_ptr 0) (crucible_term Xi);
  crucible_points_to_untyped (crucible_elem Xi_ptr 1) (crucible_term {{ get_H ctx }});
  crucible_points_to_untyped (crucible_elem Xi_ptr 2) (crucible_term {{ get_Htable ctx }});

  crucible_execute_func [in_ptr, out_ptr, crucible_term len64, key_ptr, ivec_ptr, Xi_ptr];

  // Assert that the function returns the number of bytes processed
  crucible_return (crucible_term {{ res_len64 }});

  // Assert the result values for ivec_ptr and Xi_ptr.  ivec_ptr only changes in
  // the low bytes representing the counter.
  // NB big-endian counter
  crucible_points_to ivec_ptr (crucible_term {{ iv # (split res_ctr) }});

  // the "H" and "Htable" values are constant.
  crucible_points_to_untyped (crucible_elem Xi_ptr 1) (crucible_term {{ get_H ctx }});
  crucible_points_to_untyped (crucible_elem Xi_ptr 2) (crucible_term {{ get_Htable ctx }});

  // NB, aes_hw_ctr32 works on BLOCKS
  let out_data = {{ aes_hw_ctr32_encrypt_blocks_array in_ key ivec_bytes 0 (res_len64/16) out_ }};

  llvm_setup_with_tag "Xi postcondition"
   (if eval_bool {{ enc == 0 }} then
      // for decryption, hash the input (which is the ciphertext)
      crucible_points_to_untyped (crucible_elem Xi_ptr 0)
        (crucible_term {{ cipher_update_Xi_array ctx in_ 0 res_len64 ctx.Xi }})
    else
      // for encryption, hash the output (which is the ciphertext)
      crucible_points_to_untyped (crucible_elem Xi_ptr 0)
        (crucible_term {{ cipher_update_Xi_array ctx out_data 0 res_len64 ctx.Xi }})
   );

  llvm_setup_with_tag "output buffer postcondition"
    (crucible_points_to_array_prefix out_ptr out_data {{ res_len64 }});
};


////////////////////////////////////////////////////////////////////////////////
// Proof commands

// track %rax across function calls during x86 code discovery, resulting in
// more accuracy and less performance. This is a proof hint, and does not
// introduce any new assumptions.
add_x86_preserved_reg "rax";
enable_what4_hash_consing;

/*
 * Goal tactics.
 */
let aesni_gcm_cipher_tactic = do {
//  simplify (addsimps [aesenc_key0_0_thm, aesenc_key0_1_thm, aesenclast_thm] empty_ss);
//  simplify (addsimps [aesenc_aesenclast_thm] empty_ss);
  simplify (cryptol_ss ());
//  simplify (addsimps cipher_update_avx_thms empty_ss);
//  simplify (addsimps ctr32_encrypt_thms empty_ss);
  goal_eval_unint
    [ "pmult", "pmod", "gcm_polyval", "aes_hw_encrypt","gcm_ghash_array_6x", "gcm_init_Htable", "gcm_init_H" ];

  simplify (addsimps xor_slice_append_thms basic_ss);
  simplify (addsimps slice_slice_thms empty_ss);
  simplify (addsimps xor_slice_append_thms basic_ss);
  simplify (addsimps concat_assoc_0_thms empty_ss);
  simplify (addsimps concat_assoc_1_thms empty_ss);
  simplify (addsimps concat_assoc_2_thms empty_ss);
  simplify (addsimps [cmp_sub_thm] empty_ss);
//  w4_unint_yices ["pmult", "pmod", "gcm_polyval", "aesEncryptWithKeySchedule"];
};


// aesni_gcm_encrypt_ov <- llvm_verify_x86 m "../../build/x86/crypto/crypto_test" "aesni_gcm_encrypt"
//   [ ("aesni_gcm_encrypt", 1200) // we need .Lbswap_mask, which lives in .text after aesni_gcm_encrypt (1081 bytes itself). 1200 bytes is an arbitrary size that I guessed would be large enough to contain the right bytes after alignment.
//   ]
//   true
//   (aesni_gcm_cipher_spec {{ 1 : [32] }}) // NB enc = 1 is the spec for encryption
//   aesni_gcm_cipher_tactic;

// aesni_gcm_decrypt_ov <- llvm_verify_x86 m "../../build/x86/crypto/crypto_test" "aesni_gcm_decrypt"
//   [ ("aesni_gcm_encrypt", 1200) // we need .Lbswap_mask, which lives in .text after aesni_gcm_encrypt (1081 bytes itself). 1200 bytes is an arbitrary size that I guessed would be large enough to contain the right bytes after alignment.
//   ]
//   true
//   (aesni_gcm_cipher_spec {{ 0 : [32] }}) // NB enc = 0 is the spec for decryption
//   aesni_gcm_cipher_tactic;

ghash_6x_unfolding_lemma <-
  prove_print
    do {
      unfolding ["ecEq","gcm_ghash_array_6x"];
      simplify (addsimp_shallow (core_thm "fix_unfold") empty_ss);
      simplify (cryptol_ss ());
      unfolding ["gcm_ghash_array_6x_unfold","gcm_ghash_array_6x"];
      simplify (addsimp (core_thm "boolEq_same") (cryptol_ss ()));
      trivial;
    }
    (rewrite (cryptol_ss ())
     {{ \A H Xi inp i blocks ->
           (A == gcm_ghash_array_6x H Xi inp i blocks) ==
           (A == gcm_ghash_array_6x_unfold H Xi inp i blocks)
     }});

gcm_enc_blocks_6x_unfolding_lemma <-
  prove_print
    do {
      unfolding ["ecEq","gcm_enc_blocks_6x"];
      simplify (addsimp_shallow (core_thm "fix_unfold") empty_ss);
      simplify (cryptol_ss ());
      unfolding ["gcm_enc_blocks_6x_unfold","gcm_enc_blocks_6x"];
      simplify (addsimp (core_thm "boolEq_same") (cryptol_ss ()));
      trivial;
    }
    (rewrite (cryptol_ss ())
     {{ \A in ctr key buf i blocks ->
           (arrayEq A (gcm_enc_blocks_6x in ctr key buf i blocks)) ==
           (arrayEq A (gcm_enc_blocks_6x_unfold in ctr key buf i blocks))
     }});

load_sawcore_from_file "LemmaStatements.sawcore";

let prove_lemma nm unfolds =
  prove_extcore (w4_unint_z3 [])
    (beta_reduce_term
      (unfold_term (concat [nm] unfolds) (parse_core_mod "LemmaStatements" nm)));

Xi_ite_decompose_left_thm <-
  prove_lemma "Xi_ite_decompose_left" [];

Xi_ite_decompose_right_thm <-
  prove_lemma "Xi_ite_decompose_right" [];

gcm_ghash_6x_congruence_tm <- congruence_for {{ gcm_ghash_array_6x }};
gcm_ghash_6x_congruence <-
  prove_extcore (w4_unint_z3 ["gcm_ghash_array_6x"]) gcm_ghash_6x_congruence_tm;

let {{
  type invariantTuple =
    (
    // Output buffer
    Array [64] [8]

    // stack values
    , [64],[64],[64],[64],[64],[64],[64],[64],[64],[64]
    , [128],[64],[64]

    // XMM registers
    , [512],[512],[512],[512],[512],[512],[512],[512],[512],[512],[512]

    // general purpose registers
    , [64],[64],[64],[64],[64],[64]
    )

  aesni_gcm_encrypt_invariant :
    [64] ->
    [32][8] ->
    [16][8] ->
    [16][8] ->
    Array [64] [8] ->
    invariantTuple ->
    invariantTuple ->
    Bool
  aesni_gcm_encrypt_invariant
    // values grabbed from the function specification precondition
    len64 key ivec Xi in

    // Initial values at the loop head
    ( i_buf

    // stack values
    , _, _, _, _, _, _, _, _, _, _
    , _, _, _

    // XMM registers
    , _, _, _, _, _, _, _, _, _, _, _

    // GP registers
    , _, _, _, _, _, i_blocks)

    // current values at the loop head
    ( c_buf

    // stack values
    , _, _  // I think these two correspond to scratch space, and don't need an invariant

    , c_prefetch9, c_prefetch8, c_prefetch7, c_prefetch6
    , c_prefetch5, c_prefetch4, c_prefetch3, c_prefetch2

    , c_spill_Z3 // spill location for $Z3 = %xmm7
                 // part of the "modulo-scheduled" computation for Xi

    , c_prefetch1, c_prefetch0

    // XMM registers
    , c_rndkey, c_inout5, c_inout4, c_inout3, c_inout2
    , c_inout1, c_inout0, c_Xi, c_Z3, c_Z0, c_T1

    // GP registers
    , c_a, c_b, c_c, c_d, c_counter, c_blocks) =

      // as the loop progresses, the current value of "blocks" decreases
      // from the initial value, and they always differ by a multiple of 6
      (  i_blocks >= c_blocks /\ processed_blocks%6 == 0

      /\ c_a == offset
      /\ c_b == offset + 192
      /\ c_c == offset + 192
      /\ c_d == offset + 192

      // c_counter = $counter, it implements the 32-bit counter and is used to keep track of
      // when the top byte overflows so that fixup steps are needed. Only the top byte
      // is updated, as the only purpose of this counter is to track when overflows occur.

      /\ c_counter == zext (join (reverse (drop`{12} ivec)) + (drop`{32} (processed_blocks + 12) << 24))

      // The first 16 bytes of the key are stored in this XMM register
      // at the begining of the loop

      /\ c_rndkey == zext (join (reverse (take`{16} key)))

      // The values for the IV/counter are stored in these
      // collection of XMM registers

      /\ c_T1 == zext iv

      /\ c_inout5 == zext (iv + (0x05 # zero))
      /\ c_inout4 == zext (iv + (0x04 # zero))
      /\ c_inout3 == zext (iv + (0x03 # zero))
      /\ c_inout2 == zext (iv + (0x02 # zero))
      /\ c_inout1 == zext (iv + (0x01 # zero))

      /\ c_inout0 == c_T1 ^ c_rndkey // xor key iv

      // The data to process in this iteration has already been prefetched
      // and byte-swapped into locations on the stack, or in the case of I[5],
      // into a register

      /\ c_prefetch0  == join (arrayRangeLookup`{n=8} c_buf (offset + zext 0x00))
      /\ c_prefetch1  == join (arrayRangeLookup`{n=8} c_buf (offset + zext 0x08))
      /\ c_prefetch2  == join (arrayRangeLookup`{n=8} c_buf (offset + zext 0x10))
      /\ c_prefetch3  == join (arrayRangeLookup`{n=8} c_buf (offset + zext 0x18))
      /\ c_prefetch4  == join (arrayRangeLookup`{n=8} c_buf (offset + zext 0x20))
      /\ c_prefetch5  == join (arrayRangeLookup`{n=8} c_buf (offset + zext 0x28))
      /\ c_prefetch6  == join (arrayRangeLookup`{n=8} c_buf (offset + zext 0x30))
      /\ c_prefetch7  == join (arrayRangeLookup`{n=8} c_buf (offset + zext 0x38))
      /\ c_prefetch8  == join (arrayRangeLookup`{n=8} c_buf (offset + zext 0x40))
      /\ c_prefetch9  == join (arrayRangeLookup`{n=8} c_buf (offset + zext 0x48))

      // NB, this is an entire 128-bit block
      /\ c_Z3 == zext (join (arrayRangeLookup`{n=16} c_buf (offset + zext 0x50)))

      )

   where processed_blocks = i_blocks - c_blocks
         offset = processed_blocks * 16
         end = len64 - 96

	 iv : [128]
	 iv = join (reverse (take`{12} ivec # split ctr))

	 ctr : [32]
	 ctr = i_ctr + drop`{32} processed_blocks + 12

	 i_ctr : [32]
	 i_ctr = join (drop`{12} ivec)


  aesni_gcm_decrypt_invariant :
    [64] ->
    [32][8] ->
    [16][8] ->
    [16][8] ->
    Array [64] [8] ->
    invariantTuple ->
    invariantTuple ->
    Bool
  aesni_gcm_decrypt_invariant
    // values grabbed from the function specification precondition
    len64 key ivec Xi in

    // Initial values at the loop head
    ( i_buf

    // stack values
    , _, _, _, _, _, _, _, _, _, _
    , _, _, _

    // XMM registers
    , _, _, _, _, _, _, _, _, _, _, _

    // GP registers
    , _, _, _, _, _, i_blocks)

    // current values at the loop head
    ( c_buf

    // stack values
    , _, _  // I think these two correspond to scratch space, and don't need an invariant

    , c_prefetch9, c_prefetch8, c_prefetch7, c_prefetch6
    , c_prefetch5, c_prefetch4, c_prefetch3, c_prefetch2

    , c_spill_Z3 // spill location for $Z3 = %xmm7
                 // part of the "modulo-scheduled" computation for Xi

    , c_prefetch1, c_prefetch0

    // XMM registers
    , c_rndkey, c_inout5, c_inout4, c_inout3, c_inout2
    , c_inout1, c_inout0, c_Xi, c_Z3, c_Z0, c_T1

    // GP registers
    , c_a, c_b, c_c, c_d, c_counter, c_blocks) =

      // as the loop progresses, the current value of "blocks" decreases
      // from the initial value, and they always differ by a multiple of 6

      (  i_blocks >= c_blocks /\ processed_blocks%6 == 0

      // These registers represent indices into various arrays
      // Mostly, they increase in lockstep with the block count.
      // However, the c_a register runs ahead for prefetch, except on
      // the final iteration, where it is equal to the others.

      /\ (if offset <= end then c_a == offset else c_a+96 == offset)
      /\ c_b == offset
      /\ c_c == offset
      /\ c_d == offset

      // c_counter = $counter, it implements the 32-bit counter and is used to keep track of
      // when the top byte overflows so that fixup steps are needed. Only the top byte
      // is updated, as the only purpose of this counter is to track when overflows occur.

      /\ c_counter == zext (join (reverse (drop`{12} ivec)) + (drop`{32} processed_blocks << 24))

      // The first 16 bytes of the key are stored in this XMM register
      // at the begining of the loop

      /\ c_rndkey == zext (join (reverse (take`{16} key)))

      // The values for the IV/counter are stored in these
      // collection of XMM registers

      /\ c_T1 == zext iv

      /\ c_inout5 == zext (iv + (0x05 # zero))
      /\ c_inout4 == zext (iv + (0x04 # zero))
      /\ c_inout3 == zext (iv + (0x03 # zero))
      /\ c_inout2 == zext (iv + (0x02 # zero))
      /\ c_inout1 == zext (iv + (0x01 # zero))
      /\ c_inout0 == c_T1 ^ c_rndkey // xor key iv

      // The data to process in this iteration has already been prefetched
      // and byte-swapped into locations on the stack, or in the case of I[5],
      // into a register

      /\ c_prefetch0  == join (arrayRangeLookup`{n=8} in (offset + zext 0x00))
      /\ c_prefetch1  == join (arrayRangeLookup`{n=8} in (offset + zext 0x08))
      /\ c_prefetch2  == join (arrayRangeLookup`{n=8} in (offset + zext 0x10))
      /\ c_prefetch3  == join (arrayRangeLookup`{n=8} in (offset + zext 0x18))
      /\ c_prefetch4  == join (arrayRangeLookup`{n=8} in (offset + zext 0x20))
      /\ c_prefetch5  == join (arrayRangeLookup`{n=8} in (offset + zext 0x28))
      /\ c_prefetch6  == join (arrayRangeLookup`{n=8} in (offset + zext 0x30))
      /\ c_prefetch7  == join (arrayRangeLookup`{n=8} in (offset + zext 0x38))
      /\ c_prefetch8  == join (arrayRangeLookup`{n=8} in (offset + zext 0x40))
      /\ c_prefetch9  == join (arrayRangeLookup`{n=8} in (offset + zext 0x48))

      // NB, this is an entire 128-bit block
      /\ c_Z3 == zext (join (arrayRangeLookup`{n=16} in (offset + zext 0x50)))

      )

      // Main invariant regarding the output buffer
//      /\ arrayEq
//           (gcm_enc_blocks_6x in iv rndkeys i_buf 0 i_blocks)
//           (gcm_enc_blocks_6x in iv rndkeys c_buf processed_blocks i_blocks)

      // Main invariant regarding Xi

//      /\ gcm_ghash_array_6x H (join Xi) in 0 i_blocks ==
//         gcm_ghash_array_6x H current_Xi in processed_blocks i_blocks

   where processed_blocks = i_blocks - c_blocks
         offset = processed_blocks * 16
         end = len64 - 96

	 H = join (aes_hw_encrypt zero key)

         rndkeys = map stateToMsg_x86 ([rk0]#rks#[rkn])
	 (rk0,rks,rkn) = ExpandKey (join key)

	 // The value in the Xi register lags behind, so the actual
	 // current Xi value appears to be c_Xi ^ c_Z0 ^ c_spill_Z3
	 current_Xi : [128]
	 current_Xi = c_spill_Z3 ^ (drop c_Z0 ^ drop c_Xi)

	 iv : [128]
	 iv = join (reverse (take`{12} ivec # split ctr))

	 ctr : [32]
	 ctr = i_ctr + drop`{32} processed_blocks

	 i_ctr : [32]
	 i_ctr = join (drop`{12} ivec)

}};



ite_lemma_term <- parse_core " (x : Bool) -> (y : Bool) -> (z : Bool) -> EqTrue (implies x y) -> EqTrue (implies (not x) z) -> EqTrue (ite Bool x y z)";
ite_lemma <- prove_extcore z3 ite_lemma_term;


let go =
  w4_unint_yices
    ["clmul","get_Htable","aes_hw_ctr32_encrypt_blocks_array","gcm_ghash_array_6x","aes_hw_encrypt","gcm_enc_blocks_6x","aesenc","aesenclast"];



aesni_gcm_encrypt_ov <- llvm_verify_fixpoint_x86_ex
  m "../../build/x86/crypto/crypto_test" "aesni_gcm_encrypt"
  [ ("aesni_gcm_encrypt", 1200) // we need .Lbswap_mask, which lives in .text after aesni_gcm_encrypt (1081 bytes itself). 1200 bytes is an arbitrary size that I guessed would be large enough to contain the right bytes after alignment.
  ]
  true
  ("_aesni_ctr32_ghash_6x", {{ aesni_gcm_encrypt_invariant }})
  (aesni_gcm_cipher_spec {{ 1 : [32] }}) // NB enc = 1 is the spec for decryption
  do { print_goal_summary;
       loop_inv0 <- goal_has_some_tag ["initial loop invariant"];
       loop_inv  <- goal_has_some_tag ["inductive loop invariant"];
       Xi_post   <- goal_has_some_tag ["Xi postcondition"];
       out_post  <- goal_has_some_tag ["output buffer postcondition"];
       if loop_inv0 then do {
         return (run (print "<<initial loop invariant>>"));
	 go;
       } else if loop_inv then do {
         return (run (print "<<inductive loop invariant>>"));
         go;
       } else if Xi_post then do {
         return (run (print "<<Xi post>>"));
         admit "Xi post";
       } else if out_post then do {
         return (run (print "<<output buffer post>>"));
         admit "output buffer post";
       } else do {
         go;
	 // admit "skipping";
       };
    };

aesni_gcm_decrypt_ov <- llvm_verify_fixpoint_x86_ex
  m "../../build/x86/crypto/crypto_test" "aesni_gcm_decrypt"
  [ ("aesni_gcm_encrypt", 1200) // we need .Lbswap_mask, which lives in .text after aesni_gcm_encrypt (1081 bytes itself). 1200 bytes is an arbitrary size that I guessed would be large enough to contain the right bytes after alignment.
  ]
  true
  ("_aesni_ctr32_ghash_6x", {{ aesni_gcm_decrypt_invariant }})
  (aesni_gcm_cipher_spec {{ 0 : [32] }}) // NB enc = 1 is the spec for decryption
  do { print_goal_summary;
       loop_inv0 <- goal_has_some_tag ["initial loop invariant"];
       loop_inv  <- goal_has_some_tag ["inductive loop invariant"];
       Xi_post   <- goal_has_some_tag ["Xi postcondition"];
       out_post  <- goal_has_some_tag ["output buffer postcondition"];

       if loop_inv0 then do
       { return (run (print "<<initial loop invariant>>"));
         go; 
       } else if loop_inv then do
       { return (run (print "<<inductive loop invariant>>"));
         go;
       } else if Xi_post then do {
         return (run (print "<<Xi post>>"));
         admit "Xi post";
       } else if out_post then do {
         return (run (print "<<output buffer post>>"));
         admit "output buffer post";
       } else do {
         go;
	 // admit "skipping";
       };
   };


//        { // inductive loop invariant
//          unfolding ["aesni_gcm_decrypt_invariant"];
//          beta_reduce_goal;
//          simplify (cryptol_ss ());
//        	 normalize_sequent;
//        	 focus_hyp 4;
//        	 unfolding ["/\\"];
//          simplify (cryptol_ss ());

// //	 simplify (addsimp gcm_enc_blocks_6x_unfolding_lemma empty_ss);
// //	 unfolding ["gcm_enc_blocks_6x_unfold"];

//        	 simplify (addsimp ghash_6x_unfolding_lemma empty_ss);
//        	 unfolding ["gcm_ghash_array_6x_unfold"];

//        	 beta_reduce_goal;
// 	 split_goal;
//        	 focus_concl 2;
//        	 unfolding ["/\\"];
//        	 beta_reduce_goal;
//        	 split_goal;
// 	 simplify_local [4] (cryptol_ss ());
//        	 proof_subshell ();  //admit "Xi goal!"


//        if eval_bool {{ `n == 11 \/ `n == 67 \/ `n >= 82 }} then do
// //       if eval_bool {{ `n == 15 \/ `n == 67 \/ `n >= 82 }} then do
// //       if eval_bool {{ `n == 67 \/ `n >= 82 }} then do
// //       if eval_bool {{ `n >= 82 }} then do
//        { // normalize_sequent;
//          print_goal;
//        	 proof_subshell ();
//        } else do {
//          admit "skipping";

//        };

  // aesni_gcm_cipher_tactic;


disable_what4_hash_consing;
default_x86_preserved_reg;

