/*
 * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
 * SPDX-License-Identifier: Apache-2.0
 */

////////////////////////////////////////////////////////////////////////////////
// Rewrite rules and tactics specific to the AESNI-GCM proofs

let prove_ctr32_encrypt_thm gcm_len len = prove_theorem
  (do {
    w4_unint_yices ["aes_hw_encrypt"];
  })
  (rewrite (cryptol_ss ()) {{ \key iv Xi (in : [len][8]) -> ctr32_encrypt ({ key = key, iv = iv, Xi = Xi, len = `gcm_len } : AES_GCM_Ctx) in == join (map split (aesni_gcm_ctr32_encrypt_block ({ key = key, iv = iv, Xi = Xi, len = `gcm_len } : AES_GCM_Ctx) in)) }});

ctr32_encrypt_encrypt_thm <- prove_ctr32_encrypt_thm aesni_gcm_cipher_gcm_len evp_cipher_update_bulk_encrypt;
ctr32_encrypt_decrypt_thm <- prove_ctr32_encrypt_thm aesni_gcm_cipher_gcm_len evp_cipher_update_bulk_decrypt;
let ctr32_encrypt_thms =
  [ ctr32_encrypt_encrypt_thm
  , ctr32_encrypt_decrypt_thm
  ];

let prove_cipher_update_avx_thm enc gcm_len len = prove_theorem
  (do {
    unfolding ["cipher_update", "cipher_update_byte"];
    simplify (cryptol_ss ());
    simplify (addsimps ctr32_encrypt_thms empty_ss);
    simplify (addsimp gcm_pmult_pmod_thm empty_ss);
    goal_eval_unint ["pmult", "pmod", "gcm_polyval", "gcm_polyval_mul", "gcm_polyval_mul_pmult3", "gcm_polyval_mul_pmult4", "gcm_polyval_red", "gcm_polyval_red_pmult", "aes_hw_encrypt", "aesni_gcm_ctr32_encrypt_block"];
    simplify (addsimps gcm_polyval_thms empty_ss);
    simplify (addsimps [concat_assoc_0_thm] empty_ss);
    w4_unint_yices ["pmult", "pmod", "gcm_polyval", "aes_hw_encrypt"];
  })
  (rewrite (cryptol_ss ()) {{ \key iv Xi (in : [len][8]) -> (cipher_update enc ({ key = key, iv = iv, Xi = Xi, len = `gcm_len } : AES_GCM_Ctx) in).Xi == aesni_gcm_cipher enc ({ key = key, iv = iv, Xi = Xi, len = `gcm_len } : AES_GCM_Ctx) in }});


encrypt_update_avx_thm <- prove_cipher_update_avx_thm {{ 1 : [32] }} aesni_gcm_cipher_gcm_len evp_cipher_update_bulk_encrypt;
decrypt_update_avx_thm <- prove_cipher_update_avx_thm {{ 0 : [32] }} aesni_gcm_cipher_gcm_len evp_cipher_update_bulk_decrypt;
let cipher_update_avx_thms =
  [ encrypt_update_avx_thm
  , decrypt_update_avx_thm
  ];


/*
 * Goal tactics.
 */
let aesni_gcm_cipher_tactic = do {
  simplify (addsimps [aesenc_key0_0_thm, aesenc_key0_1_thm, aesenclast_thm] empty_ss);
  simplify (addsimps [aesenc_aesenclast_thm] empty_ss);
  simplify (cryptol_ss ());
  simplify (addsimps cipher_update_avx_thms empty_ss);
  simplify (addsimps ctr32_encrypt_thms empty_ss);
  goal_eval_unint ["pmult", "pmod", "gcm_polyval", "aesEncryptWithKeySchedule"];
  simplify (addsimps xor_slice_append_thms basic_ss);
  simplify (addsimps slice_slice_thms empty_ss);
  simplify (addsimps xor_slice_append_thms basic_ss);
  simplify (addsimps concat_assoc_0_thms empty_ss);
  simplify (addsimps concat_assoc_1_thms empty_ss);
  simplify (addsimps concat_assoc_2_thms empty_ss);
  simplify (addsimps [cmp_sub_thm] empty_ss);
  w4_unint_yices ["pmult", "pmod", "gcm_polyval", "aesEncryptWithKeySchedule"];
};


////////////////////////////////////////////////////////////////////////////////
// Specifications


/* The spec for the "bulk encryption" phase.
 *
 * == crypto/fipsmodule/modes/internal.h:277 ==
 * == generated-src/linux-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.S ==
 *
 * size_t aesni_gcm_encrypt(const uint8_t *in, uint8_t *out, size_t len,
 *                          const AES_KEY *key, uint8_t ivec[16], uint64_t *Xi);
 * size_t aesni_gcm_decrypt(const uint8_t *in, uint8_t *out, size_t len,
 *                          const AES_KEY *key, uint8_t ivec[16], uint64_t *Xi);
 * 
 */
let aesni_gcm_cipher_spec (enc:Term) : LLVMSetup () = do {
  // This tracks the value of the counter value that ticks up with each step of
  // the authenticated en/decrypt operation.
  ctr_bytes <- llvm_fresh_var "ctr_bytes" (llvm_array 4 (llvm_int 8));
  // NB, big-endian assumption
  let {{ ctr = join ctr_bytes }};
  llvm_precond {{ 0 < ctr }};

  len64 <- llvm_fresh_var "len" (llvm_int 64);
  llvm_precond {{ len64 < `TOTAL_MESSAGE_MAX_LENGTH }};  

  let {{ len32 = drop`{32} len64 }};

  let {{ gcm_len = ((zero#(ctr-1)):[64]) * `AES_BLOCK_SIZE }};

  // I think we need some precondition here, but this one isn't quite right...
  // llvm_precond {{ gcm_len + len64 < `TOTAL_MESSAGE_MAX_LENGTH }};

  let encT = eval_int {{ enc }};

  // Bulk encrypt requires 3*6 blocks of input. Bulk decrypt only requires 6 blocks.
  let min_size = eval_size {| (1 + 2 * encT) * 6 * AES_BLOCK_SIZE |};

  // Input plaintext bytes, length "len"
  (in_, in_ptr) <- ptr_to_fresh_array_readonly "in" {{ len64 }};

  // Output ciphertext bytes, length "len"
  out_ptr <- llvm_symbolic_alloc false 1 {{ len64 }};

  // AES key structure, containing the expanded key
  key <- fresh_aes_key_st;
  key_ptr <- crucible_alloc_readonly (llvm_struct "struct.aes_key_st");
  points_to_aes_key_st key_ptr key;

  // IVec buffer, consisting of the IV and the counter
  ivec_ptr <- crucible_alloc (llvm_array AES_BLOCK_SIZE (llvm_int 8));
  ivec <- crucible_fresh_var "ivec" (llvm_array aes_iv_len (llvm_int 8));
  crucible_points_to_untyped (crucible_elem ivec_ptr 0) (crucible_term ivec);
  crucible_points_to_untyped (crucible_elem ivec_ptr 12) (crucible_term ctr_bytes);

  // The Xi pointer, consisting of the actual current Xi value,
  // the Hashing key "H" and the auxilary Htable value.  H and
  // Htable are constant for the duration of the en/decryption operation.
  Xi_ptr <- crucible_alloc (llvm_array 14 (llvm_int 128));
  Xi <- crucible_fresh_var "Xi" (llvm_array AES_BLOCK_SIZE (llvm_int 8));
  let ctx = {{ { key = key, iv = ivec, Xi = Xi, len = gcm_len } : AES_GCM_Ctx }};

  crucible_points_to_untyped (crucible_elem Xi_ptr 2) (crucible_term {{ get_Htable ctx }});
  crucible_points_to_untyped (crucible_elem Xi_ptr 1) (crucible_term {{ get_H ctx }});
  crucible_points_to_untyped (crucible_elem Xi_ptr 0) (crucible_term Xi);

  crucible_execute_func [in_ptr, out_ptr, crucible_term len64, key_ptr, ivec_ptr, Xi_ptr];

  // Compute how much data we expect the bulk en/decryption operation to process.
  // Assert that the function returns the total number of bytes processed so far.
  let {{ bulk_len = max `min_size ((len32 / `(6 * AES_BLOCK_SIZE)) * `(6 * AES_BLOCK_SIZE))
         do_bulk  = len32 / bulk_len
	 res_len  = do_bulk * bulk_len
	 res_ctr  = ctr + (res_len / `AES_BLOCK_SIZE) }};

  // Function returns the number of blocks processed
  crucible_return (crucible_term {{ (zext res_len):[64] }});

  // Assert the result values for ivec_ptr and Xi_ptr.  ivec_ptr only changes in
  // the low bytes representing the counter, and the "H" and "Htable" values are constant.
  crucible_points_to ivec_ptr (crucible_term {{ ivec # (split res_ctr) }});
  crucible_points_to_untyped (crucible_elem Xi_ptr 2) (crucible_term {{ get_Htable ctx }});
  crucible_points_to_untyped (crucible_elem Xi_ptr 1) (crucible_term {{ get_H ctx }});

  // For now, just assert that Xi points to something
  postXi <- llvm_fresh_var "aesni-postXi" (llvm_array AES_BLOCK_SIZE (llvm_int 8));
  crucible_points_to_untyped (crucible_elem Xi_ptr 0) (crucible_term postXi);

  // TODO! reenable!
  // if eval_bool {{ `do_bulk == 0 }} then do {
  //   // If we did no bulk encryption (because there were not enough bytes) then Xi also does not change
  //   crucible_points_to_untyped (crucible_elem Xi_ptr 0) (crucible_term Xi);
  // } else do {
  //   // Otherwise Xi and the output buffer are updated with the result of the en/decrypt operation
  //   crucible_points_to_untyped out_ptr (crucible_term {{ ctr32_encrypt ctx in_seq }});
  //   crucible_points_to_untyped (crucible_elem Xi_ptr 0) (crucible_term {{ (cipher_update enc ctx in_seq).Xi }} );
  // };

};


////////////////////////////////////////////////////////////////////////////////
// Proof commands

// track %rax across function calls during x86 code discovery, resulting in
// more accuracy and less performance. This is a proof hint, and does not
// introduce any new assumptions.
add_x86_preserved_reg "rax";
enable_what4_hash_consing;

aesni_gcm_encrypt_ov <- llvm_verify_x86 m "../../build/x86/crypto/crypto_test" "aesni_gcm_encrypt"
  [ ("aesni_gcm_encrypt", 1200) // we need .Lbswap_mask, which lives in .text after aesni_gcm_encrypt (1081 bytes itself). 1200 bytes is an arbitrary size that I guessed would be large enough to contain the right bytes after alignment.
  ]
  true
  (aesni_gcm_cipher_spec {{ 1 : [32] }})
  aesni_gcm_cipher_tactic;

aesni_gcm_decrypt_ov <- llvm_verify_x86 m "../../build/x86/crypto/crypto_test" "aesni_gcm_decrypt"
  [ ("aesni_gcm_encrypt", 1200) // we need .Lbswap_mask, which lives in .text after aesni_gcm_encrypt (1081 bytes itself). 1200 bytes is an arbitrary size that I guessed would be large enough to contain the right bytes after alignment.
  ]
  true
  (aesni_gcm_cipher_spec {{ 0 : [32] }})
  aesni_gcm_cipher_tactic;

disable_what4_hash_consing;
default_x86_preserved_reg;
