
/*
 * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
 * SPDX-License-Identifier: Apache-2.0
 */

let do_prove = false;

// TODO, move this to a better place

// The GCM counter starts at 1 and we need to leave a block at the end
// for the authentication tag. This gives us a total of slightly fewer
// than 2^^32 blocks we can handle.
let TOTAL_MESSAGE_BLOCKS = eval_size {| 2^^32 - 2 |};

// This is the total number of bytes that can be in the plain/cyphertext
// for AES-GCM.
let TOTAL_MESSAGE_MAX_LENGTH = eval_size {| TOTAL_MESSAGE_BLOCKS * AES_BLOCK_SIZE |};

////////////////////////////////////////////////////////////////////////////////
// Rewrite rules and tactics specific to the AESNI-GCM proofs

// TODO! make this independent of the length!
let prove_ctr32_encrypt_thm gcm_len len = prove_theorem
  (do {
    w4_unint_yices ["aes_hw_encrypt"];
  })
  (rewrite (cryptol_ss ()) {{ \key iv Xi (in : [len][8]) -> ctr32_encrypt ({ key = key, iv = iv, Xi = Xi, len = `gcm_len } : AES_GCM_Ctx) in == join (map split (aesni_gcm_ctr32_encrypt_block ({ key = key, iv = iv, Xi = Xi, len = `gcm_len } : AES_GCM_Ctx) in)) }});

// TODO! reenable these proofs
let ctr32_encrypt_thms = [];

// ctr32_encrypt_encrypt_thm <- prove_ctr32_encrypt_thm aesni_gcm_cipher_gcm_len evp_cipher_update_bulk_encrypt;
// ctr32_encrypt_decrypt_thm <- prove_ctr32_encrypt_thm aesni_gcm_cipher_gcm_len evp_cipher_update_bulk_decrypt;
// let ctr32_encrypt_thms =
//   [ ctr32_encrypt_encrypt_thm
//   , ctr32_encrypt_decrypt_thm
//   ];

// TODO, generalize lengths...
let prove_cipher_update_avx_thm enc gcm_len len = prove_theorem
  (do {
    unfolding ["cipher_update", "cipher_update_byte"];
    simplify (cryptol_ss ());
    simplify (addsimps ctr32_encrypt_thms empty_ss);
    simplify (addsimp gcm_pmult_pmod_thm empty_ss);
    goal_eval_unint ["pmult", "pmod", "gcm_polyval", "gcm_polyval_mul", "gcm_polyval_mul_pmult3", "gcm_polyval_mul_pmult4", "gcm_polyval_red", "gcm_polyval_red_pmult", "aes_hw_encrypt", "aesni_gcm_ctr32_encrypt_block"];
    simplify (addsimps gcm_polyval_thms empty_ss);
    simplify (addsimps [concat_assoc_0_thm] empty_ss);
    w4_unint_yices ["pmult", "pmod", "gcm_polyval", "aes_hw_encrypt"];
  })
  (rewrite (cryptol_ss ()) {{ \key iv Xi (in : [len][8]) -> (cipher_update enc ({ key = key, iv = iv, Xi = Xi, len = `gcm_len } : AES_GCM_Ctx) in).Xi == aesni_gcm_cipher enc ({ key = key, iv = iv, Xi = Xi, len = `gcm_len } : AES_GCM_Ctx) in }});


// TODO Reenable these proofs!
let cipher_update_avx_thms = [];

// encrypt_update_avx_thm <- prove_cipher_update_avx_thm {{ 1 : [32] }} aesni_gcm_cipher_gcm_len evp_cipher_update_bulk_encrypt;
// decrypt_update_avx_thm <- prove_cipher_update_avx_thm {{ 0 : [32] }} aesni_gcm_cipher_gcm_len evp_cipher_update_bulk_decrypt;
// let cipher_update_avx_thms =
//   [ encrypt_update_avx_thm
//   , decrypt_update_avx_thm
//   ];

let pg = print_goal_depth 15;

////////////////////////////////////////////////////////////////////////////////
// Specifications

let {{
  // Compute how many bytes of data we expect the bulk en/decryption operation to process.
  // This only really works with the bit sizes given because the input length
  // is assumed to be less than 2^^31
  bulk_result_length : [32] -> [64] -> [32]
  bulk_result_length min_size input_len = res_len
    where
     len32 = drop`{32} input_len
     bulk_len = max min_size ((len32 / `(6 * AES_BLOCK_SIZE)) * `(6 * AES_BLOCK_SIZE))
     do_bulk  = len32 / bulk_len
     res_len  = do_bulk * bulk_len
}};

/* The spec for the "bulk encryption" phase.
 *
 * == crypto/fipsmodule/modes/internal.h:277 ==
 * == generated-src/linux-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.S ==
 *
 * size_t aesni_gcm_encrypt(const uint8_t *in, uint8_t *out, size_t len,
 *                          const AES_KEY *key, uint8_t ivec[16], uint64_t *Xi);
 * size_t aesni_gcm_decrypt(const uint8_t *in, uint8_t *out, size_t len,
 *                          const AES_KEY *key, uint8_t ivec[16], uint64_t *Xi);
 *
 * The enc parameter determines if this is the encryption or decryption specification.
 * enc = 1 means encryption; enc = 0 means decryption.
 */
let aesni_gcm_cipher_spec (enc:Term) : LLVMSetup () =
  llvm_setup_with_tag "AESNI GCM cipher spec"
  do {

  // the "len" value is a signed 32-bit value upcasted to
  // an unsigned 64-bit value. Here we assert that the value
  // fits into the positive portion of a signed 32-bit integer.
  len64 <- llvm_fresh_var "len64" (llvm_int 64);
  llvm_precond {{ len64 < 2^^31 }};

  let encT = eval_int {{ enc }};

  // Bulk encrypt requires 3*6 blocks of input. Bulk decrypt only requires 6 blocks.
  let min_size = eval_size {| (1 + 2 * encT) * 6 * AES_BLOCK_SIZE |};

  // AES key structure, containing the expanded key
  key <- fresh_aes_key_st;
  key_ptr <- crucible_alloc_readonly (llvm_struct "struct.aes_key_st");
  points_to_aes_key_st key_ptr key;

  // IVec buffer, consisting of the IV and the block counter
  ivec_ptr <- crucible_alloc (llvm_array AES_BLOCK_SIZE (llvm_int 8));
  ivec_bytes <- crucible_fresh_var "ivec" (llvm_array AES_BLOCK_SIZE (llvm_int 8));
  crucible_points_to ivec_ptr (crucible_term ivec_bytes);

  Xi <- crucible_fresh_var "Xi" (llvm_array AES_BLOCK_SIZE (llvm_int 8));

  let {{ // NB, big-endian counter
         ctr = join (drop`{12} ivec_bytes)
	 iv  = take`{12} ivec_bytes

	 // NB, the GCM counter runs ahead
	 gcm_len = ((zext (ctr-2)):[64]) * `AES_BLOCK_SIZE

         ctx = { key = key, iv = iv , Xi = Xi, len = gcm_len } : AES_GCM_Ctx

         H = join (aes_hw_encrypt zero key)

	 res_len = bulk_result_length `min_size len64
     	 res_len64 = (zext res_len):[64]
	 res_ctr = ctr + (res_len / `AES_BLOCK_SIZE) }};

  // Input plaintext bytes, length "len"
  (in_, in_ptr) <- ptr_to_fresh_array_readonly "in" {{ res_len64 }};

  // Output ciphertext bytes, length "len"
  (out_, out_ptr) <- ptr_to_fresh_array "out" {{ res_len64 }};

  llvm_setup_with_tag "size constraints" do {
    llvm_precond {{ 1 < ctr }};
    llvm_precond {{ gcm_len <= `TOTAL_MESSAGE_MAX_LENGTH }};
    llvm_precond {{ gcm_len + len64 <= `TOTAL_MESSAGE_MAX_LENGTH }};
  };

  // The Xi pointer, consisting of the actual current Xi value,
  // the Hashing key "H" and the auxilary Htable value.  H and
  // Htable are constant for the duration of the en/decryption operation.
  Xi_ptr <- crucible_alloc (llvm_array 14 (llvm_int 128));

  crucible_points_to_untyped (crucible_elem Xi_ptr 0) (crucible_term Xi);
  crucible_points_to_untyped (crucible_elem Xi_ptr 1) (crucible_term {{ H }});
  crucible_points_to_untyped (crucible_elem Xi_ptr 2) (crucible_term {{ get_Htable ctx }});

  crucible_execute_func [in_ptr, out_ptr, crucible_term len64, key_ptr, ivec_ptr, Xi_ptr];

  // Assert that the function returns the number of bytes processed
  crucible_return (crucible_term {{ res_len64 }});

  // Assert the result values for ivec_ptr and Xi_ptr.  ivec_ptr only changes in
  // the low bytes representing the counter.
  // NB big-endian counter
  crucible_points_to ivec_ptr (crucible_term {{ iv # (split res_ctr) }});

  // the "H" and "Htable" values are constant.
  crucible_points_to_untyped (crucible_elem Xi_ptr 1) (crucible_term {{ H }});
  crucible_points_to_untyped (crucible_elem Xi_ptr 2) (crucible_term {{ get_Htable ctx }});

  // NB, aes_hw_ctr32 works on BLOCKS
//  let out_data = {{ aes_hw_ctr32_encrypt_blocks_array in_ key ivec_bytes 0 (res_len64/16) out_ }};
  let out_data = {{ gcm_enc_blocks_6x in_ iv ctr
                      (aes_round_keys key)
		      out_ 0 (res_len64/16) }};

  llvm_setup_with_tag "Xi postcondition"
   (if eval_bool {{ enc == 0 }} then
      // for decryption, hash the input (which is the ciphertext)
      crucible_points_to_untyped (crucible_elem Xi_ptr 0)
        (crucible_term {{ split`{16,8} (gcm_ghash_array_6x H (join Xi) in_ 0 (res_len64/16)) }})
    else
      // for encryption, hash the output (which is the ciphertext)
      crucible_points_to_untyped (crucible_elem Xi_ptr 0)
        (crucible_term {{ split`{16,8} (gcm_ghash_array_6x_enc_final H (join Xi) out_data (res_len64/16)) }})
   );

  llvm_setup_with_tag "output buffer postcondition"
    (crucible_points_to_array_prefix out_ptr out_data {{ res_len64 }});
};


////////////////////////////////////////////////////////////////////////////////
// Proof commands

// track %rax across function calls during x86 code discovery, resulting in
// more accuracy and less performance. This is a proof hint, and does not
// introduce any new assumptions.
add_x86_preserved_reg "rax";
enable_what4_hash_consing;

/*
 * Goal tactics.
 */
let aesni_gcm_cipher_tactic = do {
//  simplify (addsimps [aesenc_key0_0_thm, aesenc_key0_1_thm, aesenclast_thm] empty_ss);
//  simplify (addsimps [aesenc_aesenclast_thm] empty_ss);
  simplify (cryptol_ss ());
//  simplify (addsimps cipher_update_avx_thms empty_ss);
//  simplify (addsimps ctr32_encrypt_thms empty_ss);
  goal_eval_unint
    [ "pmult", "pmod", "gcm_polyval", "aes_hw_encrypt","gcm_ghash_array_6x", "gcm_init_Htable", "gcm_init_H" ];

  simplify (addsimps xor_slice_append_thms basic_ss);
  simplify (addsimps slice_slice_thms empty_ss);
  simplify (addsimps xor_slice_append_thms basic_ss);
  simplify (addsimps concat_assoc_0_thms empty_ss);
  simplify (addsimps concat_assoc_1_thms empty_ss);
  simplify (addsimps concat_assoc_2_thms empty_ss);
  simplify (addsimps [cmp_sub_thm] empty_ss);
//  w4_unint_yices ["pmult", "pmod", "gcm_polyval", "aesEncryptWithKeySchedule"];
};


let go_unint_less =
 [ "pmod","pmult","aes_hw_encrypt"
 , "aes_hw_ctr32_encrypt_blocks_array"
 , "gcm_ghash_array_internal","gcm_ghash_array_6x"
 , "gcm_enc_blocks_6x"
 , "aesenc","aesenclast","gcm_init_Htable","gcm_init_H"
 ];

let go_unint = concat go_unint_less ["ExpandKey"];

let go_less = w4_unint_yices go_unint_less;
let go = w4_unint_yices go_unint;



// aesni_gcm_encrypt_ov <- llvm_verify_x86 m "../../build/x86/crypto/crypto_test" "aesni_gcm_encrypt"
//   [ ("aesni_gcm_encrypt", 1200) // we need .Lbswap_mask, which lives in .text after aesni_gcm_encrypt (1081 bytes itself). 1200 bytes is an arbitrary size that I guessed would be large enough to contain the right bytes after alignment.
//   ]
//   true
//   (aesni_gcm_cipher_spec {{ 1 : [32] }}) // NB enc = 1 is the spec for encryption
//   aesni_gcm_cipher_tactic;

// aesni_gcm_decrypt_ov <- llvm_verify_x86 m "../../build/x86/crypto/crypto_test" "aesni_gcm_decrypt"
//   [ ("aesni_gcm_encrypt", 1200) // we need .Lbswap_mask, which lives in .text after aesni_gcm_encrypt (1081 bytes itself). 1200 bytes is an arbitrary size that I guessed would be large enough to contain the right bytes after alignment.
//   ]
//   true
//   (aesni_gcm_cipher_spec {{ 0 : [32] }}) // NB enc = 0 is the spec for decryption
//   aesni_gcm_cipher_tactic;


let {{
  type invariantTuple =
    (
    // Output buffer
    Array [64] [8]

    // stack values
    , [64],[64],[64],[64],[64],[64],[64],[64],[64],[64]
    , [128],[64],[64]

    // XMM registers
    , [512],[512],[512],[512],[512],[512],[512],[512],[512],[512],[512]

    // general purpose registers
    , [64],[64],[64],[64],[64],[64]
    )

  aesni_gcm_encrypt_invariant :
    [64] ->
    [32][8] ->
    [16][8] ->
    [16][8] ->
    Array [64] [8] ->
    invariantTuple ->
    invariantTuple ->
    Bool
  aesni_gcm_encrypt_invariant
    // values grabbed from the function specification precondition
    len64 key ivec Xi in

    // Initial values at the loop head
    ( i_buf

    // stack values
    , _, _, _, _, _, _, _, _, _, _
    , _, _, _

    // XMM registers
    , _, _, _, _, _, _, _, _, _, _, _

    // GP registers
    , _, _, _, _, _, i_blocks)

    // current values at the loop head
    ( c_buf

    // stack values
    , _, _  // I think these two correspond to scratch space, and don't need an invariant

    , c_prefetch9, c_prefetch8, c_prefetch7, c_prefetch6
    , c_prefetch5, c_prefetch4, c_prefetch3, c_prefetch2

    , c_spill_Z3 // spill location for $Z3 = %xmm7
                 // part of the "modulo-scheduled" computation for Xi

    , c_prefetch1, c_prefetch0

    // XMM registers
    , c_rndkey, c_inout5, c_inout4, c_inout3, c_inout2
    , c_inout1, c_inout0, c_Xi, c_Z3, c_Z0, c_T1

    // GP registers
    , c_a, c_b, c_c, c_d, c_counter, c_blocks) =

      // as the loop progresses, the current value of "blocks" decreases
      // from the initial value, and they always differ by a multiple of 6
      (( i_blocks >= c_blocks /\ processed_blocks%6 == 0

      // to get to the loop head at all, we must have at least
      // 6 blocks left to process

      /\ (c_blocks + 6)*16 <= len64

      /\ c_a == offset
      /\ c_b == offset + 192
      /\ c_c == offset + 192
      /\ c_d == offset + 192

      // c_counter = $counter, it implements the 32-bit counter and is used to keep track of
      // when the top byte overflows so that fixup steps are needed. Only the top byte
      // is updated, as the only purpose of this counter is to track when overflows occur.

      /\ c_counter == zext (join (reverse (drop`{12} ivec)) + (drop`{32} (processed_blocks + 12) << 24))

      // The first 16 bytes of the key are stored in this XMM register
      // at the begining of the loop

      /\ c_rndkey == zext (join (reverse (take`{16} key)))

      // The values for the IV/counter are stored in these
      // collection of XMM registers

      /\ c_T1 == zext iv

      /\ c_inout5 == zext (iv + (0x05 # zero))
      /\ c_inout4 == zext (iv + (0x04 # zero))
      /\ c_inout3 == zext (iv + (0x03 # zero))
      /\ c_inout2 == zext (iv + (0x02 # zero))
      /\ c_inout1 == zext (iv + (0x01 # zero))

      /\ c_inout0 == c_T1 ^ c_rndkey // xor key iv

      // The data to process in this iteration has already been prefetched
      // and byte-swapped into locations on the stack, or in the case of I[5],
      // into a register

      /\ c_prefetch0  == join (arrayRangeLookup`{n=8} out_data_varying (offset + zext 0x00))
      /\ c_prefetch1  == join (arrayRangeLookup`{n=8} out_data_varying (offset + zext 0x08))
      /\ c_prefetch2  == join (arrayRangeLookup`{n=8} out_data_varying (offset + zext 0x10))
      /\ c_prefetch3  == join (arrayRangeLookup`{n=8} out_data_varying (offset + zext 0x18))
      /\ c_prefetch4  == join (arrayRangeLookup`{n=8} out_data_varying (offset + zext 0x20))
      /\ c_prefetch5  == join (arrayRangeLookup`{n=8} out_data_varying (offset + zext 0x28))
      /\ c_prefetch6  == join (arrayRangeLookup`{n=8} out_data_varying (offset + zext 0x30))
      /\ c_prefetch7  == join (arrayRangeLookup`{n=8} out_data_varying (offset + zext 0x38))
      /\ c_prefetch8  == join (arrayRangeLookup`{n=8} out_data_varying (offset + zext 0x40))
      /\ c_prefetch9  == join (arrayRangeLookup`{n=8} out_data_varying (offset + zext 0x48))

      // NB, this is an entire 128-bit block
      /\ c_Z3 == zext (join (arrayRangeLookup`{n=16} out_data_varying (offset + zext 0x50)))

      // These are just facts about the way that key expansion works,
      // and don't vary across loop iterations.  We establish them in
      // the initial loop invariant so that we can treat the key
      // expansion function as uninterpreted when establishing the
      // invariant inductively.

      /\ rndkeys@0 == join (reverse (take`{16} key))
      /\ rndkeys@1 == join (reverse (drop`{16} key))
      )

      // ====== ENCRYPT INVARIANT =======
      /\ arrayEq out_data_fixed out_data_varying
      )

      // NB, note about the conjunction here... important for the split_goal below

      // Main invariant regarding Xi. Subtract 12 from the total because the final two groups
      // of six blocks are handled separately after the loop.

      /\ gcm_ghash_array_6x H (join Xi)  out_data_fixed 0 (total_blocks - 18) ==
         gcm_ghash_array_6x H current_Xi out_data_fixed processed_blocks (total_blocks - 18)

   where processed_blocks = i_blocks - c_blocks
         offset = processed_blocks * 16

	 H = join (aes_hw_encrypt zero key)

         rndkeys = aes_round_keys key

	 // The value in the Xi register lags behind, so the actual
	 // current Xi value appears to be c_Xi ^ c_Z0 ^ c_spill_Z3
	 current_Xi : [128]
	 current_Xi = c_spill_Z3 ^ (drop c_Z0 ^ drop c_Xi)

         out_data_fixed   = gcm_enc_blocks_6x in (take`{12} ivec) i_ctr rndkeys i_buf 12 total_blocks
         out_data_varying = gcm_enc_blocks_6x in (take`{12} ivec) ctr rndkeys c_buf (processed_blocks + 12) total_blocks

	 // NB, while decryption requires only a minimum of 6 blocks,
	 // the bulk encryption algorithm requires 18 blocks minimum.
	 total_blocks =
	   zext (bulk_result_length (18 * `AES_BLOCK_SIZE) len64) / `AES_BLOCK_SIZE

	 iv : [128]
	 iv = join (reverse (take`{12} ivec # split ctr))

	 ctr : [32]
	 ctr = i_ctr + drop`{32} processed_blocks

	 i_ctr : [32]
	 i_ctr = join (drop`{12} ivec) + 12


  aesni_gcm_decrypt_invariant :
    [64] ->
    [32][8] ->
    [16][8] ->
    [16][8] ->
    Array [64] [8] ->
    invariantTuple ->
    invariantTuple ->
    Bool
  aesni_gcm_decrypt_invariant
    // values grabbed from the function specification precondition
    len64 key ivec Xi in

    // Initial values at the loop head
    ( i_buf

    // stack values
    , _, _, _, _, _, _, _, _, _, _
    , _, _, _

    // XMM registers
    , _, _, _, _, _, _, _, _, _, _, _

    // GP registers
    , _, _, _, _, _, i_blocks)

    // current values at the loop head
    ( c_buf

    // stack values
    , _, _  // I think these two correspond to scratch space, and don't need an invariant

    , c_prefetch9, c_prefetch8, c_prefetch7, c_prefetch6
    , c_prefetch5, c_prefetch4, c_prefetch3, c_prefetch2

    , c_spill_Z3 // spill location for $Z3 = %xmm7
                 // part of the "modulo-scheduled" computation for Xi

    , c_prefetch1, c_prefetch0

    // XMM registers
    , c_rndkey, c_inout5, c_inout4, c_inout3, c_inout2
    , c_inout1, c_inout0, c_Xi, c_Z3, c_Z0, c_T1

    // GP registers
    , c_a, c_b, c_c, c_d, c_counter, c_blocks) =

      // as the loop progresses, the current value of "blocks" decreases
      // from the initial value, and they always differ by a multiple of 6

      ( i_blocks >= c_blocks /\ processed_blocks%6 == 0

      // to get to the loop head at all, we must have at least
      // 6 blocks left to process

      /\ (c_blocks + 6)*16 <= len64

      // These registers represent indices into various arrays
      // Mostly, they increase in lockstep with the block count.
      // However, the c_a register runs ahead for prefetch, except on
      // the final iteration, where it is equal to the others.

      /\ (if offset <= end then c_a == offset else c_a+96 == offset)
      /\ c_b == offset
      /\ c_c == offset
      /\ c_d == offset

      // c_counter = $counter, it implements the 32-bit counter and is used to keep track of
      // when the top byte overflows so that fixup steps are needed. Only the top byte
      // is updated, as the only purpose of this counter is to track when overflows occur.

      /\ c_counter == zext (join (reverse (drop`{12} ivec)) + (drop`{32} processed_blocks << 24))

      // The first 16 bytes of the key are stored in this XMM register
      // at the begining of the loop

      /\ c_rndkey == zext (join (reverse (take`{16} key)))

      // The values for the IV/counter are stored in these
      // collection of XMM registers

      /\ c_T1 == zext iv

      /\ c_inout5 == zext (iv + (0x05 # zero))
      /\ c_inout4 == zext (iv + (0x04 # zero))
      /\ c_inout3 == zext (iv + (0x03 # zero))
      /\ c_inout2 == zext (iv + (0x02 # zero))
      /\ c_inout1 == zext (iv + (0x01 # zero))
      /\ c_inout0 == c_T1 ^ c_rndkey // xor key iv

      // The data to process in this iteration has already been prefetched
      // and byte-swapped into locations on the stack, or in the case of I[5],
      // into a register

      /\ c_prefetch0  == join (arrayRangeLookup`{n=8} in (offset + zext 0x00))
      /\ c_prefetch1  == join (arrayRangeLookup`{n=8} in (offset + zext 0x08))
      /\ c_prefetch2  == join (arrayRangeLookup`{n=8} in (offset + zext 0x10))
      /\ c_prefetch3  == join (arrayRangeLookup`{n=8} in (offset + zext 0x18))
      /\ c_prefetch4  == join (arrayRangeLookup`{n=8} in (offset + zext 0x20))
      /\ c_prefetch5  == join (arrayRangeLookup`{n=8} in (offset + zext 0x28))
      /\ c_prefetch6  == join (arrayRangeLookup`{n=8} in (offset + zext 0x30))
      /\ c_prefetch7  == join (arrayRangeLookup`{n=8} in (offset + zext 0x38))
      /\ c_prefetch8  == join (arrayRangeLookup`{n=8} in (offset + zext 0x40))
      /\ c_prefetch9  == join (arrayRangeLookup`{n=8} in (offset + zext 0x48))

      // NB, this is an entire 128-bit block
      /\ c_Z3 == zext (join (arrayRangeLookup`{n=16} in (offset + zext 0x50)))

      // These are just facts about the way that key expansion works,
      // and don't vary across loop iterations.  We establish them in
      // the initial loop invariant so that we can treat the key
      // expansion function as uninterpreted when establishing the
      // invariant inductively.

      /\ rndkeys@0 == join (reverse (take`{16} key))
      /\ rndkeys@1 == join (reverse (drop`{16} key))
      )

      // Main invariant regarding the output buffer

      /\ arrayEq
           (gcm_enc_blocks_6x in (take`{12} ivec) i_ctr rndkeys i_buf 0 total_blocks)
           (gcm_enc_blocks_6x in (take`{12} ivec) ctr rndkeys c_buf processed_blocks total_blocks)

      // Main invariant regarding Xi.

      /\ gcm_ghash_array_6x H (join Xi) in 0 total_blocks ==
         gcm_ghash_array_6x H current_Xi in processed_blocks total_blocks

      // ======= DECRYPT INVARIANT ============


   where processed_blocks = i_blocks - c_blocks
         offset = processed_blocks * 16
         end = len64 - 96

	 total_blocks =
	   zext (bulk_result_length (6 * `AES_BLOCK_SIZE) len64) / `AES_BLOCK_SIZE

	 H = join (aes_hw_encrypt zero key)

         rndkeys = aes_round_keys key

	 // The value in the Xi register lags behind, so the actual
	 // current Xi value appears to be c_Xi ^ c_Z0 ^ c_spill_Z3
	 current_Xi : [128]
	 current_Xi = c_spill_Z3 ^ (drop c_Z0 ^ drop c_Xi)

	 iv : [128]
	 iv = join (reverse (take`{12} ivec # split ctr))

	 ctr : [32]
	 ctr = i_ctr + drop`{32} processed_blocks

	 i_ctr : [32]
	 i_ctr = join (drop`{12} ivec)

}};



// enable_sequent_goals;


let eqTrue x = term_apply (parse_core "EqTrue") [x];


let {{
  lemma_func :
    [12][128] ->

    [128] ->

    [6][128] ->
    [128]
  lemma_func Htable Xi_in Blocks =
     (A ^ B ^ C ^ D ^ E ^ F ^ (clmul Lpoly X) ^ (0 # S_lo) ^ (X#O))
    where
     Lpoly = 0xc200000000000000
     N = (clmul Lpoly R_hi) ^ (R_hi # R_lo)
     O#X = N

     A = clmul H1 B1_lo
     B = clmul H2 B2_lo
     C = clmul H3 B3_lo
     D = clmul H4 B4_lo
     E = clmul H5 B5_lo
     F = clmul H6 U_lo

     R_hi, R_lo, S_hi, S_lo, U_hi, U_lo : [64]

     R_lo # R_hi = R
     S_lo # S_hi = S
     U_lo # U_hi = U

     R = T ^ (S_hi # 0)

     H1  # H7  = Htable@0
     H2  # H8  = Htable@1
     H14 # H13 = Htable@2
     H3  # H9  = Htable@3
     H4  # H10 = Htable@4
     H16 # H15 = Htable@5
     H5 # H11  = Htable@6
     H6 # H12  = Htable@7
     H18 # H17 = Htable@8

     B1_lo # B1_hi = Blocks@0
     B2_lo # B2_hi = Blocks@1
     B3_lo # B3_hi = Blocks@2
     B4_lo # B4_hi = Blocks@3
     B5_lo # B5_hi = Blocks@4
     U = Xi_in ^ Blocks@5

     S2  = drop`{64} (Blocks@0 ^ (B1_lo # B1_lo))
     S5  = drop`{64} (Blocks@1 ^ (B2_lo # B2_lo))
     S8  = drop`{64} (Blocks@2 ^ (B3_lo # B3_lo))
     S11 = drop`{64} (Blocks@3 ^ (B4_lo # B4_lo))
     S14 = drop`{64} (Blocks@4 ^ (B5_lo # B5_lo))
     S17 = drop`{64} (U        ^ (U_lo  # U_lo))

     S = (clmul H7  B1_hi) ^ A ^ (clmul H13 S2)  ^
         (clmul H8  B2_hi) ^ B ^ (clmul H14 S5)  ^
         (clmul H9  B3_hi) ^ C ^ (clmul H15 S8)  ^
     	 (clmul H10 B4_hi) ^ D ^ (clmul H16 S11) ^
         (clmul H11 B5_hi) ^ E ^ (clmul H17 S14) ^
     	 (clmul H12 U_hi)  ^ F ^ (clmul H18 S17)

     T = (clmul H7  B1_hi) ^
         (clmul H8  B2_hi) ^
     	 (clmul H9  B3_hi) ^
      	 (clmul H10 B4_hi) ^
      	 (clmul H11 B5_hi) ^
      	 (clmul H12 U_hi)
}};

extra_lemma_statements <-
 do {
   Htable <- fresh_symbolic "Htable" {| [12][128] |};

   Q <- fresh_symbolic "Q" {| [128] |};

   U1 <- fresh_symbolic "U1" {| [128] |};
   U2 <- fresh_symbolic "U2" {| [128] |};

   B1_lo <- fresh_symbolic "B1_lo" {| [64] |};
   B2_lo <- fresh_symbolic "B2_lo" {| [64] |};
   B3_lo <- fresh_symbolic "B3_lo" {| [64] |};
   B4_lo <- fresh_symbolic "B4_lo" {| [64] |};
   B5_lo <- fresh_symbolic "B5_lo" {| [64] |};

   B1_hi <- fresh_symbolic "B1_hi" {| [64] |};
   B2_hi <- fresh_symbolic "B2_hi" {| [64] |};
   B3_hi <- fresh_symbolic "B3_hi" {| [64] |};
   B4_hi <- fresh_symbolic "B4_hi" {| [64] |};
   B5_hi <- fresh_symbolic "B5_hi" {| [64] |};

   B6 <- fresh_symbolic "B6" {| [128] |};

   V2  <- fresh_symbolic "V2" {| [128] |};
   V5  <- fresh_symbolic "V5" {| [128] |};
   V8  <- fresh_symbolic "V8" {| [128] |};
   V11 <- fresh_symbolic "V11" {| [128] |};
   V14 <- fresh_symbolic "V14" {| [128] |};

   let allvars = [
     Htable,
     U1,U2,

     V2,
     V5,
     V8,
     V11,
     V14,

     B1_lo,B2_lo,B3_lo,B4_lo,B5_lo,
     B1_hi,B2_hi,B3_hi,B4_hi,B5_hi,
     B6,

     Q];

   let Hlo i = term_apply (parse_core "slice Bool 0 64 64")
                [(term_apply (parse_core "at 12 (Vec 128 Bool)")
		  [Htable, nat_to_term i])];

   let Hhi i = term_apply (parse_core "slice Bool 64 64 0")
                [(term_apply (parse_core "at 12 (Vec 128 Bool)")
		  [Htable, nat_to_term i])];

   let H1  = Hlo 0;
   let H2  = Hlo 1;
   let H3  = Hlo 3;
   let H4  = Hlo 4;
   let H5  = Hlo 6;
   let H6  = Hlo 7;
   let H7  = Hhi 0;
   let H8  = Hhi 1;
   let H9  = Hhi 3;
   let H10 = Hhi 4;
   let H11 = Hhi 6;
   let H12 = Hhi 7;

   let H13 = Hhi 2;
   let H14 = Hlo 2;
   let H15 = Hhi 5;
   let H16 = Hlo 5;
   let H17 = Hhi 8;
   let H18 = Hlo 8;

   let U = {{ U1 ^ B6 ^ U2 }};
   let U_hi = term_apply (parse_core "slice Bool 64 64 0") [U];
   let U_lo = term_apply (parse_core "slice Bool 0 64 64") [U];

   let A = {{ clmul H1 B1_lo }};
   let B = {{ clmul H2 B2_lo }};
   let C = {{ clmul H3 B3_lo }};
   let D = {{ clmul H4 B4_lo }};
   let E = {{ clmul H5 B5_lo }};
   let F = {{ clmul H6 U_lo }};

   let Lpoly = {{ 0xc200000000000000 }};

   let W17 = term_apply (parse_core "slice Bool 0 64 64")
              [ {{ U1 ^ B6 ^ U2 }} ];

   let S2 = term_apply (parse_core "slice Bool 64 64 0")
              [ {{ V2 ^ (B1_lo # B1_lo)  }} ];
   let S5 = term_apply (parse_core "slice Bool 64 64 0")
              [ {{ V5 ^ (B2_lo # B2_lo )  }} ];
   let S8 = term_apply (parse_core "slice Bool 64 64 0")
              [ {{ V8 ^ (B3_lo # B3_lo)  }} ];
   let S11 = term_apply (parse_core "slice Bool 64 64 0")
              [ {{ V11 ^ (B4_lo # B4_lo)  }} ];
   let S14 = term_apply (parse_core "slice Bool 64 64 0")
              [ {{ V14 ^ (B5_lo # B5_lo )  }} ];
   let S17 = term_apply (parse_core "slice Bool 64 64 0")
              [ {{ (U1 ^ B6 ^ U2) ^ (W17 # W17 )  }} ];

   let S = {{ (clmul H7  B1_hi) ^ A ^ (clmul H13 S2)  ^
              (clmul H8  B2_hi) ^ B ^ (clmul H14 S5)  ^
              (clmul H9  B3_hi) ^ C ^ (clmul H15 S8)  ^
	      (clmul H10 B4_hi) ^ D ^ (clmul H16 S11) ^
              (clmul H11 B5_hi) ^ E ^ (clmul H17 S14) ^
	      (clmul H12 U_hi)  ^ F ^ (clmul H18 S17)
           }};

   let S_hi = term_apply (parse_core "slice Bool 64 64 0") [S];
   let S_lo = term_apply (parse_core "slice Bool 0 64 64") [S];

   let T = {{ (clmul H7  B1_hi) ^
              (clmul H8  B2_hi) ^
	      (clmul H9  B3_hi) ^
	      (clmul H10 B4_hi) ^
	      (clmul H11 B5_hi) ^
	      (clmul H12 U_hi) }};

   let R = {{ T ^ (S_hi # 0) }};

   let R_hi = term_apply (parse_core "slice Bool 64 64 0") [R];
   let R_lo = term_apply (parse_core "slice Bool 0 64 64") [R];

   let N = {{ (clmul Lpoly R_hi) ^ (R_hi # R_lo) }};
   let X = term_apply (parse_core "slice Bool 64 64 0") [N];
   let O = term_apply (parse_core "slice Bool 0 64 64") [N];

   let t1   = eqTrue {{ lemma_func Htable
                         (U1 ^ U2)
                         [ B1_lo # B1_hi
			 , B2_lo # B2_hi
			 , B3_lo # B3_hi
			 , B4_lo # B4_hi
			 , B5_lo # B5_hi
			 , B6
			 ]
                        == Q }};

   let t1'  = eqTrue {{ lemma_func Htable
                         (U1 ^ B6)
                         [ B1_lo # B1_hi
			 , B2_lo # B2_hi
			 , B3_lo # B3_hi
			 , B4_lo # B4_hi
			 , B5_lo # B5_hi
			 , U2
			 ]
                        == Q }};

   let t2   = eqTrue {{ [V2,V5,V8,V11,V14] ==
                        [ B1_lo # B1_hi
			, B2_lo # B2_hi
			, B3_lo # B3_hi
			, B4_lo # B4_hi
			, B5_lo # B5_hi
			]
                     }};

   let tend = eqTrue {{ (A ^ B ^ C ^ D ^ E ^ F ^
                             (clmul Lpoly X) ^ (0 # S_lo) ^ (X # O)) == Q }};
   let tfinal  = implies_term t2 (implies_term t1 tend);
   let tfinal' = implies_term t2 (implies_term t1' tend);

   let l1 = (generalize_term allvars
      (rewrite (cryptol_ss ()) (unfold_term ["ecEq"] tfinal)));
   let l2 = (generalize_term allvars
      (rewrite (cryptol_ss ()) (unfold_term ["ecEq"] tfinal')));

   return (l1,l2);
 };

extra_lemma1 <-
  prove_extcore
  do { go; // admit "extra_lemm1";
     }
  (extra_lemma_statements.0);

extra_lemma2 <-
  prove_extcore
  do { go; // admit "extra_lemma2";
     }
  (extra_lemma_statements.1);

func_lemma_cong_stmt <- congruence_for {{ lemma_func }};
func_lemma_cong <-
  prove_extcore (w4_unint_z3 ["lemma_func"]) func_lemma_cong_stmt;

func_lemma_eq <-
  prove_print
  do { admit "func_lemma_eq TODO!!"; }
  {{ \Htable Xi blocks ->
       lemma_func Htable Xi blocks ==
       (aesni_gcm_cipher_block6 True Htable Xi (reverse blocks))
  }};

// This is a horribly gross hack.  Basically, I'm constructing
// a lemma here whose sole purpose is to restate the Xi
// postcondition of the encryption algorithm sufficently
// that the solver can directly show that the result of simulation
// matches.
//
// To that end, the conclusion of this lemma requires an
// implication involving loop invariant hypothesis. This is primarily
// a way to get enough values into scope here that I can use them
// to write down the intermediate values I need. Without doing something
// like this, it's very difficult to even figure out how to write down
// what I want to say. Essentialy, we are using pattern matching via
// the lemma conclusion to give names to the values we want.
//
// The proof of this lemma is where all the interesting reasoning
// actually goes to use the loop invariant, unfold recursive
// definitions, etc.

enc_Xi_post_lemma_statement <-
  do {
    b <- fresh_symbolic "b" {| Bool |};
    Xother <- fresh_symbolic "Xother" {| [8] |};
    X  <- fresh_symbolic "X" {| [128] |};

    let Xslice = term_apply (parse_core "slice Bool 0 8 120") [X];
    let A0 = {{ if b then Xother else Xslice }};

    A1 <- fresh_symbolic "A" {| [8] |};
    A2 <- fresh_symbolic "A" {| [8] |};
    A3 <- fresh_symbolic "A" {| [8] |};
    A4 <- fresh_symbolic "A" {| [8] |};
    A5 <- fresh_symbolic "A" {| [8] |};
    A6 <- fresh_symbolic "A" {| [8] |};
    A7 <- fresh_symbolic "A" {| [8] |};
    A8 <- fresh_symbolic "A" {| [8] |};
    A9 <- fresh_symbolic "A" {| [8] |};
    A10 <- fresh_symbolic "A" {| [8] |};
    A11 <- fresh_symbolic "A" {| [8] |};
    A12 <- fresh_symbolic "A" {| [8] |};
    A13 <- fresh_symbolic "A" {| [8] |};
    A14 <- fresh_symbolic "A" {| [8] |};
    A15 <- fresh_symbolic "A" {| [8] |};

    let AS = [b,X,Xother,A1,A2,A3,A4,A5,A6,A7,A8,A9,A10,A11,A12,A13,A14,A15];

    H  <- fresh_symbolic "H" {| [128] |};
    blocks <- fresh_symbolic "blocks" {| [64] |};

    len64 <- fresh_symbolic "len64" {| [64] |};
    key   <- fresh_symbolic "key"   {| [32][8] |};
    ivec  <- fresh_symbolic "ivec"  {| [16][8] |};
    Xi    <- fresh_symbolic "Xi"    {| [16][8] |};
    inp   <- fresh_symbolic "inp"   {| Array [64] [8] |};
    buf   <- fresh_symbolic "buf"   {| Array [64] [8] |};

    start_ivec <- fresh_symbolic "start_ivec" {| [12][8] |};
    start_ctr  <- fresh_symbolic "start_ctr"  {| [32] |};

    i_tuple <- fresh_symbolic "i_tuple" {| invariantTuple |};
    c_tuple <- fresh_symbolic "c_tuple" {| invariantTuple |};

    let allvars = concat AS
      [ H, Xi, blocks, len64, key, ivec, inp, buf
      , start_ivec, start_ctr
      , i_tuple, c_tuple
      ];

    let {{
           ( i_buf

           // stack values
           , _, _, _, _, _, _, _, _, _, _
           , _, _, _

           // XMM registers
           , _, _, _, _, _, _, _, _, _, _, _

           // GP registers
           , _, _, _, _, _, i_blocks) = i_tuple

           ( c_buf

           // stack values
           , _, _  // I think these two correspond to scratch space, and don't need an invariant

           , c_prefetch9, c_prefetch8, c_prefetch7, c_prefetch6
           , c_prefetch5, c_prefetch4, c_prefetch3, c_prefetch2

           , c_spill_Z3 // spill location for $Z3 = %xmm7
                        // part of the "modulo-scheduled" computation for Xi

           , c_prefetch1, c_prefetch0

           // XMM registers
           , c_rndkey, c_inout5, c_inout4, c_inout3, c_inout2
           , c_inout1, c_inout0, c_Xi, c_Z3, c_Z0, c_T1

           // GP registers
           , c_a, c_b, c_c, c_d, c_counter, c_blocks) = c_tuple

           processed_blocks = i_blocks - c_blocks
           offset = processed_blocks * 16

           rndkeys = aes_round_keys key

           // The value in the Xi register lags behind, so the actual
           // current Xi value appears to be c_Xi ^ c_Z0 ^ c_spill_Z3
           current_Xi : [128]
           current_Xi = c_spill_Z3 ^ (drop c_Z0 ^ drop c_Xi)

           iv : [128]
           iv = join (reverse (take`{12} ivec # split ctr))

           ctr : [32]
           ctr = i_ctr + drop`{32} processed_blocks

           i_ctr : [32]
           i_ctr = join (drop`{12} ivec) + 12

	   // NB, while decryption requires only a minimum of 6 blocks,
	   // the bulk encryption algorithm requires 18 blocks minimum.
	   total_blocks =
	     zext (bulk_result_length (18 * `AES_BLOCK_SIZE) len64) / `AES_BLOCK_SIZE
    }};

    let first_blocks = {{
       	         [ (c_prefetch0 # c_prefetch1)
		 , (c_prefetch2 # c_prefetch3)
		 , (c_prefetch4 # c_prefetch5)
		 , (c_prefetch6 # c_prefetch7)
		 , (c_prefetch8 # c_prefetch9)
		 , drop`{512-128} c_Z3
		 ]
    }};

    let mid_blocks = {{
       [ join (arrayRangeLookup c_buf (c_a + 96 + j*16 )) | j <- [5,4,3,2,1,0] ]:[6][128]
    }};

    let last_blocks = {{
       [ swap8
           ((enc_block (join (reverse (take`{12} ivec # split (ctr+j)))) rndkeys)
            ^
            join (reverse (arrayRangeLookup inp (c_c + (zext j)*16))))
       | j <- [ 5, 4, 3, 2, 1, 0:[32] ]
       ]
    }};

    let enc_final_restated =
      {{ (if blocks >= 18 then
           lemma_func (gcm_init_Htable (gcm_init_H H))
             (lemma_func (gcm_init_Htable (gcm_init_H H))
               (aesni_gcm_cipher_block6 False (gcm_init_Htable (gcm_init_H H))
                 current_Xi
                 first_blocks)
               mid_blocks)
             last_blocks
          else
           (join Xi))
      }};

    let out_data         = {{ gcm_enc_blocks_6x inp start_ivec start_ctr (aes_round_keys key) buf 0 blocks }};
    let out_data_fixed   = {{ gcm_enc_blocks_6x inp (take`{12} ivec) i_ctr rndkeys i_buf 12 total_blocks }};
    let out_data_varying = {{ gcm_enc_blocks_6x inp (take`{12} ivec) ctr rndkeys c_buf (processed_blocks + 12) total_blocks }};

    let t1 = eqTrue {{ [A0,A1,A2,A3,A4,A5,A6,A7,A8,A9,A10,A11,A12,A13,A14,A15] == split X }};

    let t2 = eqTrue {{ X == enc_final_restated }};

    let t3a = eqTrue {{ last_blocks  == [ join (arrayRangeLookup out_data_varying ((blocks-6+j) * 16 ))  | j <- [5,4,3,2,1,0] ] }};
    let t3b = eqTrue {{ mid_blocks   == [ join (arrayRangeLookup out_data_varying ((blocks-12+j) * 16 )) | j <- [5,4,3,2,1,0] ] }};
    let t3c = eqTrue {{ first_blocks == [ join (arrayRangeLookup out_data_varying ((blocks-18+j) * 16 )) | j <- [0 .. 5] ] }};
    let t3d = eqTrue {{ arrayEq out_data_fixed out_data }};
    let t3e = eqTrue {{ current_Xi   == gcm_ghash_array_6x H (join Xi) out_data_fixed 0 (blocks-18) }};

    let tend = eqTrue
         (term_apply (parse_core "implies")
	 [ {{ aesni_gcm_encrypt_invariant len64 key ivec Xi inp i_tuple c_tuple }}
         , {{ [A0,A1,A2,A3,A4,A5,A6,A7,A8,A9,A10,A11,A12,A13,A14,A15]
                 == split (gcm_ghash_array_6x_enc_final H (join Xi)
                            out_data
		            blocks) }}
	 ]);

    let tfinal =
      (implies_term t1 (implies_term t2
         (implies_term t3a (implies_term t3b (implies_term t3c
	   (implies_term t3d (implies_term t3e tend)))))));

    return (generalize_term allvars
      (rewrite (cryptol_ss ()) (unfold_term ["ecEq"] tfinal)));
  };


split_16_8_cong_stmt <- congruence_for (parse_core "split 16 8 Bool");
split_16_8_cong <- prove_extcore z3 split_16_8_cong_stmt;

aesni_gcm_cipher_block6_cong_stmt <- congruence_for {{ aesni_gcm_cipher_block6 }};
aesni_gcm_cipher_block6_cong <-
  prove_extcore (w4_unint_z3 ["aesni_gcm_cipher_block6"])
  aesni_gcm_cipher_block6_cong_stmt;

enc_Xi_post_lemma <-
  prove_extcore
  do {
    simplify (addsimp func_lemma_eq empty_ss);
    w4_unint_yices (concat go_unint ["aesni_gcm_cipher_block6"]);
  }
  enc_Xi_post_lemma_statement;



aesni_gcm_encrypt_ov <- llvm_verify_x86_with_invariant
  m "../../build/x86/crypto/crypto_test" "aesni_gcm_encrypt"
  [ ("aesni_gcm_encrypt", 1200) // we need .Lbswap_mask, which lives in .text after aesni_gcm_encrypt (1081 bytes itself). 1200 bytes is an arbitrary size that I guessed would be large enough to contain the right bytes after alignment.
  ]
  true
  ("_aesni_ctr32_ghash_6x", 0, {{ aesni_gcm_encrypt_invariant }})
  (aesni_gcm_cipher_spec {{ 1 : [32] }}) // NB enc = 1 is the spec for decryption

  // ==== ENCRYPT PROOF ====

  do { print_goal_summary;
       loop_inv0 <- goal_has_some_tag ["initial loop invariant"];
       loop_inv  <- goal_has_some_tag ["inductive loop invariant"];
       Xi_post   <- goal_has_some_tag ["Xi postcondition"];
       out_post  <- goal_has_some_tag ["output buffer postcondition"];
       if loop_inv0 then do {
         return (run (print "<<initial loop invariant>>"));

	 unfolding ["aesni_gcm_encrypt_invariant"];
	 simplify (cryptol_ss ());
	 simplify (addsimps [ arrayRange_gcm_enc_blocks_6x_lemma_8
	                    , arrayRange_gcm_enc_blocks_6x_lemma_16
			    ] empty_ss);
	 go_less;
//admit "skipping";

       } else if loop_inv then do {
         return (run (print "<<inductive loop invariant>>"));

         normalize_sequent;

	 unfolding ["aesni_gcm_encrypt_invariant","ecEq","/\\"];
	 simplify (cryptol_ss ());
	 simplify (addsimp aesenclast_thm basic_ss);

	 focus_concl 2; // focus on loop invariant conclusion
	 split_goal;

	 // first prove the necessary facts about the ghash
	 focus_hyp 4;
	 simplify (addsimp ghash_6x_unfolding_lemma empty_ss);
	 go;
//	 admit "skipping";

	 // now that's done, prove the rest of the invariant
	 focus_hyp 4;
	 simplify (addsimp gcm_enc_blocks_6x_unfolding_lemma empty_ss);
	 focus_hyp 5;
	 simplify (addsimps [ arrayRange_gcm_enc_blocks_6x_lemma_8
	                    , arrayRange_gcm_enc_blocks_6x_lemma_16
	                    ] empty_ss);
         go;
//	 admit "skipping";

       } else if Xi_post then do {
         return (run (print "<<Xi post>>"));

	 normalize_sequent;
	 simplify (addsimp aesenclast_thm (cryptol_ss ()));

	 focus_hyp 0;
	 split_goal;
	 go;

	 normalize_sequent;
	 focus_concl 1;
	 goal_revert_hyp 1;

	 // This lemma is applied to decompose this proof into
	 // smaller subgoals, as the whole postcondition proof
	 // is too large/complex to handle reasonably otherwise.
	 //
	 // The following 7 subgoals arise from appying this lemma.
	 goal_apply enc_Xi_post_lemma;

	 // (1) boring/easy structural properties
	 go;

	 // (2) Show that the term computed in symbolic simulation
	 // matches the restatement from the enc_Xi_post_lemma.
	 //
	 // This is wierdly difficult, and requires several
	 // annoying intermediate lemmas that are hyper-specific
	 // to the terms generated by the symbolic simulator.
	 // The intermediate function we are using here to restate
	 // this lemma was supposed to be basically identical to the
	 // term generated, so it feels very strange that this is so
	 // difficult.
	 //
	 // It would be very nice to find a better way to do this.
	 simplify (addsimp custom_append_lemma empty_ss);
	 goal_apply bv128_ite_decompose_right;
	 goal_intro_hyps 1;

	 goal_apply extra_lemma1;
	 go;
	 goal_apply func_lemma_cong;
	 go;
	 goal_apply extra_lemma2;
	 go;
	 goal_apply func_lemma_cong;
	 go;
	 go;
	 go;
	 go;
	 go;

	 // (3) final block equalities.
	 // Here we need to show that the AES counter mode encrypted
	 // blocks computed in the final iteration of the loop match
	 // the expected values of the final 6 blocks of output buffer.
	 simplify (addsimp gcm_enc_blocks_6x_unfolding_lemma2 empty_ss);
	 unfolding ["gcm_enc_blocks_6x_unfold"];
	 simplify (cryptol_ss ());
	 simplify (addsimp_shallow gcm_enc_blocks_6x_test_unfold_lemma empty_ss);
	 w4_unint_yices (concat go_unint ["enc_block"]);

	 // (4) mid block equalities
	 // Here we need to show that blocks looked up in the "current output buffer"
	 // are the ones expected. This is similar to one of the steps we need to
	 // do when establishing the loop invariant.
	 simplify (addsimp_shallow arrayRange_gcm_enc_blocks_6x_lemma_16 empty_ss);
	 go;

	 // (5) first block equalities
	 // Here, we simply need to show that the "prefetched" block data
	 // at the loop head matches the expected values.  This is pretty much
	 // directly stated by the loop invariant.
	 go;

	 // (6) initial pipeline stage proof
	 // Here, we have to show that the initial state of the output buffer
	 // at the loop header matches the expected values. This involves reasoning
	 // about the 12 blocks encrypted at the beginning of the function before
	 // the loop begins.

	 // unfold once
	 simplify (addsimp gcm_enc_blocks_6x_unfolding_lemma empty_ss);
	 unfolding ["gcm_enc_blocks_6x_unfold"];
	 simplify (cryptol_ss ());
	 goal_apply arrayEq_ite_decompose_right;
	 goal_intro_hyps 1;
	 // unfold again
	 simplify (addsimp gcm_enc_blocks_6x_unfolding_lemma empty_ss);
	 unfolding ["gcm_enc_blocks_6x_unfold"];
	 simplify (cryptol_ss ());
	 goal_apply arrayEq_ite_decompose_right;
	 goal_intro_hyps 1;

	 // prove the main equation
	 go;
	 // clean up the other cases
	 go;
	 go;

	 // (7) current Xi equality
	 // Here, we need to show that the current Xi value at the loop head
	 // corresponds as required to the ghash value we are computing.  This
	 // basically involves manipulating the loop invariant to show that
	 // the recursive ghash function (shifted back by 18) terminates along
	 // with the loop condition.
	 focus_hyp 1;
	 unfolding ["aesni_gcm_encrypt_invariant","ecEq","/\\"];
	 simplify (cryptol_ss ());
	 simplify (addsimp ghash_6x_unfolding_lemma empty_ss);
	 go;

       } else if out_post then do {
         return (run (print "<<output buffer post>>"));

	 normalize_sequent;
         simplify (cryptol_ss ());

	 // first thing we want to do is unroll the gcm_enc_block_6x
	 // function twice to handle the two initial groups of 6 blocks
	 // that are encrypted before the loop starts
         focus_concl 0;
           simplify (addsimp_shallow gcm_enc_blocks_6x_unfolding_lemma2 empty_ss);
           unfolding ["gcm_enc_blocks_6x_unfold"];
           simplify (cryptol_ss ());
           simplify (addsimp_shallow gcm_enc_blocks_6x_unfolding_lemma2 empty_ss);
           unfolding ["gcm_enc_blocks_6x_unfold"];
           simplify (cryptol_ss ());
         unfocus;

         // now we focus on the loop invariant. We also need to unfold it once to
         // deal with the final loop iteration.
         focus_hyp 0;
           unfolding ["aesni_gcm_encrypt_invariant","ecEq","/\\"];
	   simplify (cryptol_ss ());
	   simplify (addsimp_shallow gcm_enc_blocks_6x_unfolding_lemma empty_ss);
           unfolding ["gcm_enc_blocks_6x_unfold"];
           simplify (cryptol_ss ());
         unfocus;

         simplify (addsimp aesenclast_thm basic_ss);
         simplify (addsimp_shallow gcm_enc_blocks_6x_test_unfold_lemma empty_ss);

	 go;

//         admit "output buffer post";
       } else do {
         go;
	 // admit "skipping";
       };
    };


aesni_gcm_decrypt_ov <- llvm_verify_x86_with_invariant
  m "../../build/x86/crypto/crypto_test" "aesni_gcm_decrypt"
  [ ("aesni_gcm_encrypt", 1200) // we need .Lbswap_mask, which lives in .text after aesni_gcm_encrypt (1081 bytes itself). 1200 bytes is an arbitrary size that I guessed would be large enough to contain the right bytes after alignment.
  ]
  true
  ("_aesni_ctr32_ghash_6x", 0, {{ aesni_gcm_decrypt_invariant }})
  (aesni_gcm_cipher_spec {{ 0 : [32] }}) // NB enc = 0 is the spec for decryption

  // ==== DECRYPT PROOF ====

  do { print_goal_summary;
       loop_inv0 <- goal_has_some_tag ["initial loop invariant"];
       loop_inv  <- goal_has_some_tag ["inductive loop invariant"];
       Xi_post   <- goal_has_some_tag ["Xi postcondition"];
       out_post  <- goal_has_some_tag ["output buffer postcondition"];

       if loop_inv0 then do
       { return (run (print "<<initial loop invariant>>"));
         go_less;
       } else if loop_inv then do
       { return (run (print "<<inductive loop invariant>>"));

         normalize_sequent;

	 focus_hyp 4; // focus on the loop induction hypothesis
  	   unfolding ["aesni_gcm_decrypt_invariant","ecEq"];
	   simplify (cryptol_ss ());
	   simplify (addsimps [ gcm_enc_blocks_6x_unfolding_lemma
	                      , ghash_6x_unfolding_lemma
	 		      ] empty_ss);
	 unfocus;

	 simplify (addsimp aesenclast_thm basic_ss);
	 go;

//	 admit "skipping";

       } else if Xi_post then do {
         return (run (print "<<Xi post>>"));

         normalize_sequent;

	 focus_hyp 0; // focus on the loop induction hypothesis
  	   unfolding ["aesni_gcm_decrypt_invariant","ecEq","/\\"];
	   simplify (cryptol_ss ());
	   simplify (addsimp ghash_6x_unfolding_lemma empty_ss);
	   unfolding ["gcm_ghash_array_6x_unfold"];
	 unfocus;

         simplify (cryptol_ss ());
         simplify (addsimp_shallow ghash_6x_test_unfold_lemma empty_ss);

	 go;
//	 admit "Xi post";
       } else if out_post then do {
         return (run (print "<<output buffer post>>"));

         normalize_sequent;

	 focus_hyp 0; // focus on the loop induction hypothesis
  	   unfolding ["aesni_gcm_decrypt_invariant","ecEq","/\\"];
	   simplify (cryptol_ss ());
	   simplify (addsimp gcm_enc_blocks_6x_unfolding_lemma empty_ss);
	   unfolding ["gcm_enc_blocks_6x_unfold"];
	 unfocus;

         simplify (cryptol_ss ());
	 simplify (addsimp aesenclast_thm basic_ss);
         simplify (addsimp_shallow gcm_enc_blocks_6x_test_unfold_lemma empty_ss);

	 go;

         // admit "output buffer post";
       } else do {
         go;
	 // admit "skipping";
       };
   };




//        	 simplify (addsimp ghash_6x_unfolding_lemma empty_ss);
//        	 unfolding ["gcm_ghash_array_6x_unfold"];

// disable_sequent_goals;
disable_what4_hash_consing;
default_x86_preserved_reg;


print "OMG! You did it!";
exit 0;
