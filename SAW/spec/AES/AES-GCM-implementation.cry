/*
 * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
 * SPDX-License-Identifier: Apache-2.0
*/

module AES_GCM_Implementation where

import Array

import Primitive::Symmetric::Cipher::Block::AES
import Primitive::Symmetric::Cipher::Authenticated::AES_256_GCM
import AES_GCM
import X86



/*
 * GCM polyval mul/red implementations
 */
gcm_polyval_mul_pmult3 : {n} (fin n) => [2 * (1 + n)] -> [2 * (1 + n)] -> [4 * (1 + n)]
gcm_polyval_mul_pmult3 X Y = r3 # (r2 ^ r1 ^ r3 ^ m1) # (r1 ^ r0 ^ r2 ^ m0) # r0
  where
    [X_hi, X_lo] = split X
    [Y_hi, Y_lo] = split Y
    [r1, r0] = split ((0 : [1]) # (pmult X_lo Y_lo))
    [r3, r2] = split ((0 : [1]) # (pmult X_hi Y_hi))
    [m1, m0] = split ((0 : [1]) # (pmult (X_lo ^ X_hi) (Y_hi ^ Y_lo)))

gcm_polyval_mul_pmult4 : {n} (fin n) => [2 * (1 + n)] -> [2 * (1 + n)] -> [4 * (1 + n)]
gcm_polyval_mul_pmult4 X Y = (hi ^ (0 : [1 + n]) # (hi_bits (m0 ^ m1))) # (lo ^ ((lo_bits (m0 ^ m1)) # (0 : [1 + n])))
  where
    [X_hi, X_lo] = split X
    [Y_hi, Y_lo] = split Y
    lo = (0 : [1]) # (pmult X_lo Y_lo)
    hi = (0 : [1]) # (pmult X_hi Y_hi)
    m0 = (0 : [1]) # (pmult X_lo Y_hi)
    m1 = (0 : [1]) # (pmult X_hi Y_lo)

gcm_polyval_red_pmult : [256] -> [128]
gcm_polyval_red_pmult X = (d1 ^ x3) # (d0 ^ x2)
  where
    [x3, x2, x1, x0] = split X
    [a1, a0] = split ((0 : [1]) # (pmult <| x^^63 + x^^62 + x^^57 |> x0))
    b1 = x0 ^ a1
    b0 = x1 ^ a0
    [c1, c0] = split ((0 : [1]) # (pmult <| x^^63 + x^^62 + x^^57 |> b0))
    d1 = b0 ^ c1
    d0 = b1 ^ c0

gcm_polyval_avx : [128] -> [128] -> [128]
gcm_polyval_avx H X = x30
  where
    [H_hi, H_lo] = split H
    [X_hi, X_lo] = split X
    x17 = (0 : [1]) # (pmult H_lo X_lo)
    x18 = ((0 : [1]) # (pmult H_hi X_hi)) ^ x17
    x19 = x18 ^ ((0 : [1]) # (pmult (H_lo ^ H_hi) (X_lo ^ X_hi)))
    x20 = (lo_bits x19) # (0 : [64])
    x21 = x17 ^ x20
    x22 = hi_bits x21
    x23 = lo_bits x21
    x25 = (take`{8} ((x23 << 57) ^ (x23 << 62) ^ (x23 << 63))) # (0 : [120])
    x26 = x21 ^ x25
    x27 = (hi_bits x26) >> 1
    x28 = (lo_bits x26) >> 1
    x29 = (x26 ^ (x27 # x28)) ^ ((x27 >> 5) # (x28 >> 5))
    x30 = ((((x18 ^ x20) ^ ((0 : [64]) # (hi_bits x19))) ^ x25) ^ ((0 : [64]) # (take`{8} ((x22 << 57) ^ (x22 << 62) ^ (x22 << 63))) # (0 : [56]))) ^ (((hi_bits x29) >> 1) # ((lo_bits x29) >> 1))


/*
 * aesni_gcm_cipher Cryptol implementation
 */
aesni_gcm_cipher : {n} (fin n) => [32] -> AES_GCM_Ctx -> [(1 + n) * 6 * 16][8] -> [16][8]
aesni_gcm_cipher enc ctx in = split Xi''
  where
    enc_blks = if enc ! 0
      then split (aesni_gcm_ctr32_encrypt_block ctx in)
      else split (map join (split in))
    Htable = get_Htable ctx
    Xi' = aesni_gcm_cipher_block6 Htable (join ctx.Xi) (enc_blks @ 0)
    Xi'' = foldl (aesni_gcm_cipher_block6 Htable) Xi' (drop`{1} enc_blks)

aesni_gcm_ctr32_encrypt_block : {n} (fin n) => AES_GCM_Ctx -> [n * 16][8] -> [n][16 * 8]
aesni_gcm_ctr32_encrypt_block ctx in = out
  where
    ctr = drop ((ctx.len / 16) + 1)
    in' = split in
    out = [ swap8 ((join (reverse blk)) ^ (join (reverse (EKi ctx (ctr + i))))) | blk <- in' | i <- [0 ...] ]

aesni_gcm_cipher_block6 : [12][128] -> [128] -> [6][128] -> [128]
aesni_gcm_cipher_block6 Htable Xi blks = Xi_6
  where
    Xi_0 = gcm_polyval_mul_pmult4 (Htable @ 0) (blks @ 5)
    Xi_1 = gcm_polyval_mul_pmult4 (Htable @ 1) (blks @ 4)
    Xi_2 = gcm_polyval_mul_pmult4 (Htable @ 3) (blks @ 3)
    Xi_3 = gcm_polyval_mul_pmult4 (Htable @ 4) (blks @ 2)
    Xi_4 = gcm_polyval_mul_pmult4 (Htable @ 6) (blks @ 1)
    Xi_5 = gcm_polyval_mul_pmult4 (Htable @ 7) (Xi ^ (blks @ 0))
    Xi_6 = gcm_polyval_red_pmult (Xi_0 ^ Xi_1 ^ Xi_2 ^ Xi_3 ^ Xi_4 ^ Xi_5)


gcm_ghash_array_internal : [128] -> [128] -> Array [64] [8] -> [64] -> [64] -> [128]
gcm_ghash_array_internal H Xi inp i blocks =
  if blocks - i >= 1 then
    gcm_ghash_array_internal H
      (gcm_polyval (gcm_init_H H) (Xi ^ (join (arrayRangeLookup inp (i*16)))))
      inp (i+1) blocks
  else
    Xi

gcm_ghash_array_internal_unfold : [128] -> [128] -> Array [64] [8] -> [64] -> [64] -> [128]
gcm_ghash_array_internal_unfold H Xi inp i blocks =
  if blocks - i >= 1 then
    gcm_ghash_array_internal H
      (gcm_polyval (gcm_init_H H) (Xi ^ (join (arrayRangeLookup inp (i*16)))))
      inp (i+1) blocks
  else
    Xi

// Here is an alternate definition that unfolds 6 times
gcm_ghash_array_6x : [128] -> [128] -> Array [64] [8] -> [64] -> [64] -> [128]
gcm_ghash_array_6x H Xi inp i blocks =
  if blocks - i >= 6 then
    gcm_ghash_array_6x H
      (aesni_gcm_cipher_block6 (gcm_init_Htable (gcm_init_H H)) Xi
        [ join (arrayRangeLookup inp ((i+j) * 16 )) | j <- [0 .. 5] ])
      inp (i+6) blocks
  else
    Xi

gcm_ghash_array_6x_unfold : [128] -> [128] -> Array [64] [8] -> [64] -> [64] -> [128]
gcm_ghash_array_6x_unfold H Xi inp i blocks =
  if blocks - i >= 6 then
    gcm_ghash_array_6x H
      (aesni_gcm_cipher_block6 (gcm_init_Htable (gcm_init_H H)) Xi
        [ join (arrayRangeLookup inp ((i+j) * 16 )) | j <- [0 .. 5] ])
      inp (i+6) blocks
  else
    Xi


gcm_enc_6x : Array [64] [8] -> [12][8] -> [32] -> [15][128] -> Array [64] [8] -> [64] -> Array [64] [8]
gcm_enc_6x in iv ctr rndkeys buf i =
  foldl
    (\arr j ->
      arrayRangeUpdate arr ((i+zext j)*16) (reverse (split
       ((enc_block (join (reverse (iv # split (ctr+j)))) rndkeys)
        ^
        join (reverse (arrayRangeLookup in ((i+zext j)*16))))))
    )
    buf
    [ 0 .. 5 ]


// This captures the computation done in the final steps of 'aesni_gcm_encrypt'
// There are 2 final groups of 6 blocks that are processed after the main loop,
// and they are computed in a slightly different way.
gcm_ghash_array_6x_enc_final : [128] -> [128] -> Array [64] [8] -> [64] -> [128]
gcm_ghash_array_6x_enc_final H Xi outp blocks =
  if blocks >= 18 then
    aesni_gcm_cipher_block6 (gcm_init_Htable (gcm_init_H H))
      (aesni_gcm_cipher_block6 (gcm_init_Htable (gcm_init_H H))
        (aesni_gcm_cipher_block6 (gcm_init_Htable (gcm_init_H H))
          (gcm_ghash_array_6x H Xi outp 0 (blocks-18))
	  [ join (arrayRangeLookup outp ((blocks-18+j) * 16 )) | j <- [0 .. 5] ])
        [ join (arrayRangeLookup outp ((blocks-12+j) * 16 )) | j <- [0 .. 5] ])
      [ join (arrayRangeLookup outp ((blocks-6+j) * 16 )) | j <- [0 .. 5] ]
  else
    Xi

enc_block : [128] -> [15][128] -> [128]
enc_block blk rndkeys =
  (aesenclast
   (aesenc
     (aesenc
      (aesenc
       (aesenc
	 (aesenc
	  (aesenc
	   (aesenc
	    (aesenc
	     (aesenc
	      (aesenc
	       (aesenc
	        (aesenc
		 (aesenc
		  (( (rndkeys@0)) ^ blk)
		  ( (rndkeys@1)))
		 ( (rndkeys@2)))
	        ( (rndkeys@3)))
	       ( (rndkeys@4)))
	      ( (rndkeys@5)))
	     ( (rndkeys@6)))
	    ( (rndkeys@7)))
	   ( (rndkeys@8)))
	  ( (rndkeys@9)))
	 ( (rndkeys@10)))
       ( (rndkeys@11)))
      ( (rndkeys@12)))
    ( (rndkeys@13)))
  ( (rndkeys@14)))


aes_round_keys : [32][8] -> [15][128]
aes_round_keys key = map stateToMsg_x86 ([rk0]#rks#[rkn])
 where
   (rk0,rks,rkn) = ExpandKey (join key)

gcm_enc_blocks_6x : Array [64] [8] -> [12][8] -> [32] -> [15][128] -> Array [64] [8] -> [64] -> [64] -> Array [64] [8]
gcm_enc_blocks_6x in iv ctr rndkeys buf i blocks =
  if blocks - i >= 6 then
    gcm_enc_blocks_6x in iv (ctr+6) rndkeys
      (gcm_enc_6x in iv ctr rndkeys buf i)
      (i+6)
      blocks
  else
    buf

gcm_enc_blocks_6x_unfold : Array [64] [8] -> [12][8] -> [32] -> [15][128] -> Array [64] [8] -> [64] -> [64] -> Array [64] [8]
gcm_enc_blocks_6x_unfold in iv ctr rndkeys buf i blocks =
  if blocks - i >= 6 then
    gcm_enc_blocks_6x in iv (ctr+6) rndkeys
      (gcm_enc_6x in iv ctr rndkeys buf i)
      (i+6)
      blocks
  else
    buf

gcm_enc : Array [64] [8] -> [12][8] -> [32] -> [15][128] -> Array [64] [8] -> [64] -> Array [64] [8]
gcm_enc in iv ctr rndkeys buf i =
   arrayRangeUpdate buf (i*16) (reverse (split
       ((enc_block (join (reverse (iv # split ctr))) rndkeys)
        ^
        join (reverse (arrayRangeLookup in (i*16))))))

gcm_enc_blocks : Array [64] [8] -> [12][8] -> [32] -> [15][128] -> Array [64] [8] -> [64] -> [64] -> Array [64] [8]
gcm_enc_blocks in iv ctr rndkeys buf i blocks =
  if blocks - i >= 1 then
    gcm_enc_blocks in iv (ctr+1) rndkeys
      (gcm_enc in iv ctr rndkeys buf i)
      (i+1)
      blocks
  else
    buf

gcm_enc_blocks_unfold : Array [64] [8] -> [12][8] -> [32] -> [15][128] -> Array [64] [8] -> [64] -> [64] -> Array [64] [8]
gcm_enc_blocks_unfold in iv ctr rndkeys buf i blocks =
  if blocks - i >= 1 then
    gcm_enc_blocks in iv (ctr+1) rndkeys
      (gcm_enc in iv ctr rndkeys buf i)
      (i+1)
      blocks
  else
    buf


gcm_ghash_avx : {n, m} (fin n, 1 <= m, m <= 8) => [128] -> [16][8] -> [(n * 8 + m) * 16][8] -> [16][8]
gcm_ghash_avx H Xi in = Xi''
  where
    bulk # tl = in
    Xi' = foldl (gcm_ghash_avx_8`{8} H) Xi (split`{n} bulk)
    Xi'' = gcm_ghash_avx_8`{m} H Xi' tl

gcm_ghash_avx_8 : {n} (1 <= n, n <= 8) => [128] -> [16][8] -> [n * 16][8] -> [16][8]
gcm_ghash_avx_8 H Xi in = Xi''
  where
    [H0, H1, _, H2, H3, _, H4, H5, _, H6, H7, _] = gcm_init_Htable H
    Htable = [H0, H1, H2, H3, H4, H5, H6, H7]
    Xi' = join Xi
    blks = map join (split in)
    Xi_0 = gcm_polyval_mul_pmult3 (Htable @ (`n - 1)) (Xi' ^ (blks @ 0))
    Xi_1 = gcm_polyval_mul_pmult3 (Htable @ (`n - 2)) (blks @ 1)
    Xi_2 = gcm_polyval_mul_pmult3 (Htable @ (`n - 3)) (blks @ 2)
    Xi_3 = gcm_polyval_mul_pmult3 (Htable @ (`n - 4)) (blks @ 3)
    Xi_4 = gcm_polyval_mul_pmult3 (Htable @ (`n - 5)) (blks @ 4)
    Xi_5 = gcm_polyval_mul_pmult3 (Htable @ (`n - 6)) (blks @ 5)
    Xi_6 = gcm_polyval_mul_pmult3 (Htable @ (`n - 7)) (blks @ 6)
    Xi_7 = gcm_polyval_mul_pmult3 (Htable @ (`n - 8)) (blks @ 7)
    T1 = if `n == 8
      then Xi_7 ^ Xi_6 ^ Xi_5 ^ Xi_4 ^ Xi_3 ^ Xi_2 ^ Xi_1 ^ Xi_0
      else if `n == 7
        then Xi_6 ^ Xi_5 ^ Xi_4 ^ Xi_3 ^ Xi_2 ^ Xi_1 ^ Xi_0
        else if `n == 6
          then Xi_5 ^ Xi_4 ^ Xi_3 ^ Xi_2 ^ Xi_1 ^ Xi_0
          else if `n == 5
            then Xi_4 ^ Xi_3 ^ Xi_2 ^ Xi_1 ^ Xi_0
            else if `n == 4
              then Xi_3 ^ Xi_2 ^ Xi_1 ^ Xi_0
              else if `n == 3
                then Xi_2 ^ Xi_1 ^ Xi_0
                else if `n == 2
                  then Xi_1 ^ Xi_0
                  else Xi_0
    Xi_n = gcm_polyval_red_pmult T1
    Xi'' = split Xi_n



update_enc_array :
  Array [64] [8] ->
  AES_GCM_Ctx ->
  Array [64] [8] ->
  [64] ->
  [64] ->
  ([64], AES_GCM_Ctx, Array [64] [8])
update_enc_array in ctx buf i len =
//    if (prephase_len != len) then
      (prephase_len + bulk_len + len_blocks, ctx2, buf2)
//    else
//      (prephase_len, ctx0, buf0)

  where
    // TODO!
    (prephase_len, ctx0, buf0) = (0, ctx, buf)
      // update_enc_prephase in ctx buf (ctx.len%16) i len

    (bulk_len, ctx1, buf1) = update_bulk_encrypt in ctx0 buf0 (i + prephase_len) (len - prephase_len)

    (len_blocks, ctx2, buf2) =
      update_postbulk_encrypt in ctx1 buf1 (i + (prephase_len + bulk_len)) (len - (prephase_len + bulk_len))

    // TODO! postphase


update_enc_prephase :
  Array [64] [8] ->
  AES_GCM_Ctx ->
  Array [64] [8] ->
  [64] ->
  [64] ->
  [64] ->
  ([64], AES_GCM_Ctx, Array [64] [8])
update_enc_prephase in0 ctx0 buf0 n0 i0 len0 =
  if (n0 > 0) then
    loop in0 ctx0 buf0 n0 0 16 i0 len0
  else
    (0, ctx0, buf0)

 where
  // bnd is a conrete upper bound to force symbolic termination
  loop in ctx buf n count bnd i len =
    if (n > 0 /\ len > 0 /\ bnd > 0) then
       loop
         in
         { key = ctx.key, iv = ctx.iv, Xi = Xi', len = ctx.len + 1 }
         buf'
         ((n+1)%16) (count+1) (bnd-1) (i+1) (len-1)
    else
      if (n == 0) then
        (count, { key = ctx.key, iv = ctx.iv, Xi = gcm_gmult H ctx.Xi, len = ctx.len }, buf)
      else
        (count, ctx, buf)

   where
     H    = gcm_init_H (join (get_H ctx))

     Xi'  = update ctx.Xi n (ctx.Xi@n ^ b)
     buf' = arrayUpdate buf i b
     b    = arrayLookup in i ^ EK_byte ctx

update_bulk_encrypt :
  Array [64] [8] ->
  AES_GCM_Ctx ->
  Array [64] [8] ->
  [64] ->
  [64] ->
  ([64], AES_GCM_Ctx, Array [64] [8])
// Precondition ctx.len is a multiple of 16
update_bulk_encrypt in ctx buf i len =

    ( res_len
    , { key = ctx.key, iv = ctx.iv, Xi = split Xi_final, len = ctx.len + res_len }
    , arrayCopy buf i out_data 0 res_len
    )

  where
    out_data = gcm_enc_blocks_6x
                  in // (arrayCopy zeroByteArray 0 in i res_len)  I think this array copy will be necessary when we enable the prephase
		  ctx.iv
		  (drop`{32} (ctx.len/16) + 2)
		  (aes_round_keys ctx.key)
		  buf // (arrayCopy zeroByteArray 0 buf i res_len)  I think this array copy will be necessary when we enable the prephase
		  0
		  (res_len/16)

    Xi_final = gcm_ghash_array_6x_enc_final
                  (join (aes_hw_encrypt zero ctx.key))
		  (join ctx.Xi)
		  out_data
		  (res_len/16)

    len32 = drop`{32} len
    bulk_len = max `(3 * 6 * 16)
                    ( (len32 / `(6 * 16)) * `(6 * 16))
    do_bulk  = len32 / bulk_len
    res_len  = zext (do_bulk * bulk_len)


update_postbulk_encrypt :
  Array [64] [8] ->
  AES_GCM_Ctx ->
  Array [64] [8] ->
  [64] ->
  [64] ->
  ([64], AES_GCM_Ctx, Array [64] [8])
// Precondition ctx.len is a multiple of 16
// len < 18*AES_BLOCK_SIZE
update_postbulk_encrypt in ctx buf i len =
  (if len_blocks == 0 then
    (0, ctx, buf)
   else
    ( len_blocks
    , { key = ctx.key, iv = ctx.iv, Xi = Xi_final, len = ctx.len + len_blocks }
    , arrayCopy buf i out_data 0 len_blocks
    )
      where
        out_data = aes_hw_ctr32_encrypt_blocks_array
                      (arrayCopy zeroByteArray 0 in i len_blocks)
	              ctx.key (get_Yi ctx) 0 (len_blocks/16)
		      (arrayCopy zeroByteArray 0 buf i len_blocks)
        Xi_final = gcm_ghash_array (gcm_init_H (join (get_H ctx)))
	              ctx.Xi
	              (arrayCopy zeroByteArray 0 (arrayCopy buf i out_data 0 len_blocks) i len_blocks)
		      0
		      (len_blocks/16)
   )

  where
    len_blocks = len && 0xFFFFFFFFFFFFFFF0

update_cleanup_encrypt :
  Array [64] [8] ->
  AES_GCM_Ctx ->
  Array [64] [8] ->
  [64] ->
  [64] ->
  [64] -> // concrete bound to force termination
  (AES_GCM_Ctx, Array [64] [8])
// Precondition ctx.len is a multiple of 16
// len < 16
update_cleanup_encrypt in ctx buf i len bnd = (ctx, buf)

  // if len != 0 /\ bnd > 0 then
  //   update_cleanup_encrypt in
  //     { key = ctx.key, iv = ctx.iv, Xi = Xi', len = ctx.len+1 }
  //     buf' (i+1) (len-1) (bnd-1)
  //    where
  //     Xi'  = update ctx.Xi n (ctx.Xi@n ^ b)
  //     buf' = arrayUpdate buf i b
  //     b    = arrayLookup in i ^ EK_byte ctx
  //     n    = ctx.len % 16

  // else
  //   (ctx, buf)



update_dec_array :
  Array [64] [8] ->
  AES_GCM_Ctx ->
  Array [64] [8] ->
  [64] ->
  [64] ->
  ([64], AES_GCM_Ctx, Array [64] [8])
update_dec_array in ctx buf i len =
//    if (prephase_len != len) then
      (prephase_len + bulk_len + len_blocks, ctx2, buf2)
//    else
//      (prephase_len, ctx0, buf0)

  where
    // TODO!
    (prephase_len, ctx0, buf0) = (0, ctx, buf)
      // update_dec_prephase in ctx buf (ctx.len%16) i len

    (bulk_len, ctx1, buf1) = update_bulk_decrypt in ctx0 buf0 (i + prephase_len) (len - prephase_len)

    (len_blocks, ctx2, buf2) =
      update_postbulk_decrypt in ctx1 buf1 (i + (prephase_len + bulk_len)) (len - (prephase_len + bulk_len))

    // TODO! postphase


update_bulk_decrypt :
  Array [64] [8] ->
  AES_GCM_Ctx ->
  Array [64] [8] ->
  [64] ->
  [64] ->
  ([64], AES_GCM_Ctx, Array [64] [8])
// Precondition ctx.len is a multiple of 16
update_bulk_decrypt in ctx buf i len =

    ( res_len
    , { key = ctx.key, iv = ctx.iv, Xi = split Xi_final, len = ctx.len + res_len }
    , arrayCopy buf i out_data 0 res_len
    )

  where
    out_data = gcm_enc_blocks_6x
                  in // (arrayCopy zeroByteArray 0 in i res_len)  I think this array copy will be necessary when we enable the prephase
		  ctx.iv
		  (drop`{32} (ctx.len/16) + 2)
		  (aes_round_keys ctx.key)
		  buf // (arrayCopy zeroByteArray 0 buf i res_len) I think this array copy will be necessary when we enable the prephase
		  0
		  (res_len/16)

    Xi_final = gcm_ghash_array_6x
                  (join (aes_hw_encrypt zero ctx.key))
		  (join ctx.Xi)
		  in
		  0
		  (res_len/16)

    len32 = drop`{32} len
    bulk_len = max `(6 * 16)
                    ( (len32 / `(6 * 16)) * `(6 * 16))
    do_bulk  = len32 / bulk_len
    res_len  = zext (do_bulk * bulk_len)


update_postbulk_decrypt :
  Array [64] [8] ->
  AES_GCM_Ctx ->
  Array [64] [8] ->
  [64] ->
  [64] ->
  ([64], AES_GCM_Ctx, Array [64] [8])
// Precondition ctx.len is a multiple of 16
// len < 18*AES_BLOCK_SIZE
update_postbulk_decrypt in ctx buf i len =
  (if len_blocks == 0 then
    (0, ctx, buf)
   else
    ( len_blocks
    , { key = ctx.key, iv = ctx.iv, Xi = Xi_final, len = ctx.len + len_blocks }
    , arrayCopy buf i out_data 0 len_blocks
    )
      where
        out_data = aes_hw_ctr32_encrypt_blocks_array
                      (arrayCopy zeroByteArray 0 in i len_blocks)
	              ctx.key (get_Yi ctx) 0 (len_blocks/16)
		      (arrayCopy zeroByteArray 0 buf i len_blocks)
        Xi_final = gcm_ghash_array (gcm_init_H (join (get_H ctx)))
	              ctx.Xi
	              (arrayCopy zeroByteArray 0 in i len_blocks)
		      0
		      (len_blocks/16)
   )

  where
    len_blocks = len && 0xFFFFFFFFFFFFFFF0
